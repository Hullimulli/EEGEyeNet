import sys
import os
import time
import logging
from typing import Union
import math
from config import config, create_folder
from utils import IOHelper
from benchmark import benchmark, split
from utils.tables_utils import print_table
from hyperparameters import our_DL_models, our_ML_models, your_models, all_models
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, log_loss
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import tensorflow as tf
import tensorflow.keras as keras
from tf_keras_vis.activation_maximization import ActivationMaximization
from tf_keras_vis import ModelVisualization
from tf_keras_vis.utils import get_num_of_steps_allowed, is_mixed_precision, listify, normalize, zoom_factor
from tf_keras_vis.utils.model_modifiers import ExtractIntermediateLayerForGradcam as ModelModifier
from tf_keras_vis.saliency import Saliency
from tf_keras_vis.activation_maximization.callbacks import Progress
from scipy.ndimage.interpolation import zoom
import shutil
import pandas as pd
import re
import cv2
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib.animation as animation
from matplotlib.ticker import FormatStrFormatter
import scipy.io as sio
from scipy.ndimage import convolve1d
import mne
from texttable import Texttable
from tabulate import tabulate
import latextable
from matplotlib.lines import Line2D

import numpy as np


class Tee(object):
    def __init__(self, *files):
        self.files = files

    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()  # If you want the output to be visible immediately

    def flush(self):
        for f in self.files:
            f.flush()

class AnalEyeZor():

    def __init__(self, task, dataset, preprocessing, models, electrodes = 1+np.arange(129),featureExtraction = False, trainBool = True, saveModelBool = True, path=None):
        """
        A class, where the task and folder location is defined.
        @param task: Problem Type, can be 'Direction_task', 'Position_task' or 'LR_task'.
        @type task: String
        @param dataset: Which data set is used, can be 'antisaccade','dots' or 'processing_speed'.
        @type dataset: String
        @param preprocessing: How the data is preprossed, can be either 'min' or 'max'.
        @type preprocessing: String
        @param models: Models, which have to be loaded or trained.
        @type models: List of Strings
        @param electrodes: Which electrodes should be used for training.
        @type electrodes: Numpy Int Array
        @param featureExtraction: If hilbert transformed data is used.
        @type featureExtraction: Bool
        @param trainBool: If the models have to be newly trained.
        @type trainBool: Bool
        @param saveModelBool: If the newly trained models have to be saved.
        @type saveModelBool: Bool
        @param path: Path to an existing folder containing the networks and .csv files as generated by AnalEyeZor.
        @type path: String
        """


        config['include_ML_models'] = False
        config['include_DL_models'] = False
        config['include_your_models'] = True
        config['include_dummy_models'] = False
        config['feature_extraction'] = featureExtraction
        self.electrodes = electrodes
        self.inputShape = (1, 258) if config['feature_extraction'] else (500, electrodes.shape[0])
        self.numberOfNetworks = 5
        self.currentFolderPath = ""
        self.modelNames = models
        config['task'] = task
        config['dataset'] = dataset
        config['preprocessing'] = preprocessing

        self.displayBoundariesX = [0,800]
        self.displayBoundariesY = [0, 600]
        self.customSignalType = ["Step","ContStep","ContStepConfused","TripleSlide","Constant","StepDirection17","StepDirection15"]

        def build_file_name():
            all_EEG_file = config['task'] + '_with_' + config['dataset']
            all_EEG_file = all_EEG_file + '_' + 'synchronised_' + config['preprocessing']
            all_EEG_file = all_EEG_file + ('_hilbert.npz' if config['feature_extraction'] else '.npz')
            return all_EEG_file

        config['all_EEG_file'] = build_file_name()
        #########################################Loading_or_Training_Model####################################################
        your_models[config['task']] = {config['dataset']:{config['preprocessing']:{}}}
        firstModel = True

        #Generating corresponding config list to be used in benchmark
        for model in models:
            if config['task'] == 'Direction_task':
                if model not in our_DL_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'] and model not in our_ML_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'] \
                and model not in our_DL_models[config['task']][config['dataset']][config['preprocessing']]['angle'] and model not in our_ML_models[config['task']][config['dataset']][config['preprocessing']]['angle']:
                    print("{} not yet configured.".format(model))
                else:
                    if firstModel:
                        your_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'] = {model: {}}
                        your_models[config['task']][config['dataset']][config['preprocessing']]['angle'] = {model: {}}
                        firstModel = False
                    else:
                        your_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][model] = {}
                        your_models[config['task']][config['dataset']][config['preprocessing']]['angle'][model] = {}
                    if model in our_DL_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'] and model in our_DL_models[config['task']][config['dataset']][config['preprocessing']]['angle']:
                        your_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][model] = \
                        our_DL_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][model]

                        your_models[config['task']][config['dataset']][config['preprocessing']]['angle'][model] = \
                        our_DL_models[config['task']][config['dataset']][config['preprocessing']]['angle'][model]
                    else:
                        your_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][model] = \
                        our_ML_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][model]
                        your_models[config['task']][config['dataset']][config['preprocessing']]['angle'][model] = \
                        our_ML_models[config['task']][config['dataset']][config['preprocessing']]['angle'][model]
                    your_models[config['task']][config['dataset']][config['preprocessing']]['angle'][model][1]["input_shape"] = self.inputShape
                    your_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][model][1]["input_shape"] = self.inputShape
            else:

                if model not in our_DL_models[config['task']][config['dataset']][config['preprocessing']] and model not in our_ML_models[config['task']][config['dataset']][config['preprocessing']]:
                    print("{} not yet configured.".format(model))
                else:
                    if firstModel:
                        your_models[config['task']][config['dataset']][config['preprocessing']] = {
                            model: {}}
                        firstModel = False
                    else:
                        your_models[config['task']][config['dataset']][config['preprocessing']][model] = {}

                    if model in our_DL_models[config['task']][config['dataset']][config['preprocessing']]:
                        your_models[config['task']][config['dataset']][config['preprocessing']][model] = \
                        our_DL_models[config['task']][config['dataset']][config['preprocessing']][model]
                    else:
                        your_models[config['task']][config['dataset']][config['preprocessing']][model] = \
                        our_ML_models[config['task']][config['dataset']][config['preprocessing']][model]
                    your_models[config['task']][config['dataset']][config['preprocessing']][model][1]["input_shape"] = self.inputShape

        all_models.pop(config['task'], None)
        all_models[config['task']] = your_models[config['task']]
        def initFolder():
            create_folder()
            logging.basicConfig(filename=config['info_log'], level=logging.INFO)
            logging.info('Started the Logging')
            logging.info("Num GPUs Available: {}".format(len(tf.config.list_physical_devices('GPU'))))
            if os.path.exists(config['model_dir'] + '/console.out'):
                f = open(config['model_dir'] + '/console.out', 'a')
            else:
                f = open(config['model_dir'] + '/console.out', 'w')
            sys.stdout = Tee(sys.stdout, f)

        if trainBool:
            config['retrain'] = trainBool
            config['save_models'] = saveModelBool
            initFolder()
            self.currentFolderPath = config['model_dir'] + "/"
            logging.info("------------------------------Loading the Data------------------------------")
            trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :, self.electrodes.astype(np.int)-1]
            trainY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1]
            logging.info("------------------------------Calling Benchmark------------------------------")
            benchmark(trainX, trainY)
            logging.info("------------------------------Finished Training------------------------------")
        elif path==None:
            self.currentFolderPath = "./runs"
            config['load_experiment_dir'] = self.currentFolderPath
        else:
            self.currentFolderPath = config['log_dir'] + path
            config['load_experiment_dir'] = path
            config['retrain'] = trainBool
            initFolder()
            logging.info("------------------------------Selected Model------------------------------")

    def PFI(self, scale = False, nameSuffix='',iterations=5, useAccuracyBool=False, saveTrail=''):
        """
        Function that uses the folder which is defined when initialised to perform PFI with all contained networks.
        @param scale: Set to true, if the input has to be scaled.
        @type scale: Bool
        @param iterations: How many times the PFI is done. Averages over all iterations.
        @type iterations: Int
        @param useAccuracyBool: If True, uses accuracy instead of bce for LR-Task scoring.
        @type useAccuracyBool: Bool
        @param saveTrail: The string, with which the save-folder begins before stating the networks name. Should be either '', '_amplitude' or '_angle'
        @type saveTrail: String
        @return: Returns the ratio of loss between the prediction when one electrode is permutated and when nothing is permutated.
        This is done for each electrode. Electrode number is the index + 1.
        @rtype: Dictionary with numpy array items
        """
        logging.info("------------------------------PFI------------------------------")
        if config['feature_extraction'] == True:
            print("No PFI for Transformed Data")
            return

        config['retrain'] = False
        config['save_models'] = False

        trainX, trainY = IOHelper.get_npz_data(config['data_dir'], verbose=True)
        dataShape = np.shape(trainX)
        ids = trainY[:, 0]
        trainIndices, valIndices, testIndices = split(ids, 0.7, 0.15, 0.15)
        if scale:
            logging.info('Standard Scaling')
            scaler = StandardScaler()
            scaler.fit(trainX[trainIndices])
            trainX = scaler.transform(trainX[valIndices])
        trainX, valY = trainX[valIndices], trainY[valIndices,1:]
        del trainIndices, valIndices, testIndices, trainY

        modelLosses = dict()
        if saveTrail == '_angle':
            models = all_models[config['task']][config['dataset']][config['preprocessing']]['angle']
            valY = valY[:, 1]
        elif saveTrail == '_amplitude':
            models = all_models[config['task']][config['dataset']][config['preprocessing']]['amplitude']
            valY = valY[:, 0]
        else:
            models = all_models[config['task']][config['dataset']][config['preprocessing']]

        for name, model in models.items():
            electrodeLosses = np.zeros(dataShape[2])
            start_time = time.time()
            offset = 0
            trainer = model[0](**model[1])
            print("Evaluating Base of {}".format(name))
            for i in range(self.numberOfNetworks):
                path = config['checkpoint_dir'] + 'run' + str(i + 1) + '/'
                #Modification in benchmark.py st. an angle network does not overwrite amplitude
                trainer.ensemble.load_file_pattern = re.compile(saveTrail + name + '_nb_*', re.IGNORECASE)
                trainer.load(path)
                #This is a work around in order to get the non rounded data of a classifier.
                trainer.type = 'regressor'
                prediction = np.squeeze(trainer.predict(trainX))

                if config['task'] == 'LR_task':
                    if useAccuracyBool:
                        prediction = np.rint(prediction)
                    offset += self.binaryCrossEntropyLoss(valY, prediction)
                elif config['task'] == 'Direction_task' and saveTrail == '_amplitude':
                    offset += self.meanSquareError(valY, prediction)
                elif config['task'] == 'Direction_task' and saveTrail == '_angle':
                    offset += self.angleError(valY, prediction)
                elif config['task'] == 'Position_task':
                    offset += self.euclideanDistance(valY, prediction)
                else:
                    print("No corresponding error function")
                    return
            offset = offset / self.numberOfNetworks

            print("Evaluating PFI of {}".format(name))
            for k in range(iterations):
                for j in tqdm(range(int(dataShape[2]))):
                    valX = trainX.copy()
                    np.random.shuffle(valX[:, :, j])
                    for i in range(self.numberOfNetworks):
                        path = config['checkpoint_dir'] + 'run' + str(i + 1) + '/'
                        trainer.ensemble.load_file_pattern = re.compile(saveTrail + name + '_nb_*', re.IGNORECASE)
                        trainer.load(path)
                        trainer.type = 'regressor'
                        prediction = np.squeeze(trainer.predict(valX))

                        if config['task'] == 'LR_task':
                            if useAccuracyBool:
                                prediction = np.rint(prediction)
                            electrodeLosses[j] += self.binaryCrossEntropyLoss(valY, prediction)
                        elif config['task'] == 'Direction_task' and saveTrail=='_amplitude':
                            electrodeLosses[j] += self.meanSquareError(valY, prediction)
                        elif config['task'] == 'Direction_task' and saveTrail=='_angle':
                            electrodeLosses[j] += self.angleError(valY, prediction)
                        elif config['task'] == 'Position_task':
                            electrodeLosses[j] += self.euclideanDistance(valY, prediction)

            modelLosses[name] = np.divide((electrodeLosses / (iterations*self.numberOfNetworks)), offset) - 1
            runtime = (time.time() - start_time)
            logging.info("--- Sorted Electrodes According to Influence for {}:".format(name))
            logging.info(1+(np.argsort(modelLosses[name]))[::-1])
            logging.info("--- Losses of each Electrode for {}:".format(name))
            logging.info(modelLosses[name][((np.argsort(modelLosses[name]))[::-1])])
            logging.info("--- Runtime: %s for seconds ---" % runtime)
        logging.info("------------------------------Evaluated Electrodes------------------------------")
        results = np.expand_dims(np.arange(1,list(modelLosses.values())[0].shape[0]+1),0).astype(np.int)
        legend = 'Electrode Number'
        for i,j in modelLosses.items():
            legend += ','+i
            results = np.concatenate((results,np.expand_dims(j,0)),axis=0)

        np.savetxt(config['model_dir'] +  'PFI' + nameSuffix + '.csv', results.transpose(), fmt='%s', delimiter=',', header=legend, comments='')
        return modelLosses

    def electrodeBarPlot(self,values,filename="Electrode_Loss",format='pdf',colour='red'):
        """

        @param values: Array of values, all values <0 are set to 0.
        @type values: Numpy Array
        @param filename: Savename for the plot.
        @type filename: String
        @param format: File format for the save file.
        @type format: String
        @param colour: Colour for a matplotlib bar plot.
        @type colour: String
        @return:
        @rtype:
        """

        values[np.where(values < 0)] = 0
        xAxis = np.arange(values.shape[0]) + 1
        fig = plt.figure()
        plt.xlabel("Electrode Number")
        plt.bar(xAxis, np.squeeze(values), color=colour)
        plt.legend()
        fig.savefig(config['model_dir'] + filename + ".{}".format(format), format=format, transparent=True)
        plt.close()

    def electrodePlot(self, colourValues, filename="Electrode_Configuration", alpha=0.4, pathForOriginalRelativeToExecutable="./Joels_Files/forPlot/"):
        """
        Colour codes the electrodes on the visualisation.
        @param colourValues: An array with the colour values which are used. Index of axis 0 is the electrode number - 1.
        @type colourValues: Numpy Array of shape [<=129,3]
        @param filename: Savename for the plot.
        @type filename: String
        @param alpha: How transparent the colours are done.
        @type alpha: Float
        @param pathForOriginalRelativeToExecutable: Depending on where main.py is started, this path as to be adjusted s.t.
        it leads from the starting file to the forPlot folder.
        @type pathForOriginalRelativeToExecutable: String
        @return:
        @rtype:
        """

        if not os.path.exists(pathForOriginalRelativeToExecutable+'blank.png'):
            print(pathForOriginalRelativeToExecutable+'blank.png'+" does not exist.")
            return
        img = cv2.imread(pathForOriginalRelativeToExecutable+'blank.png', cv2.IMREAD_COLOR)
        overlay = img.copy()
        coord = pd.read_csv(pathForOriginalRelativeToExecutable+'coord.csv', index_col='electrode', dtype=int)
        for i in range(colourValues.shape[0]):
            pt = coord.loc[i+1]
            x, y, r = pt['posX'], pt['posY'], pt['radius']
            cv2.circle(overlay, (x, y), r, colourValues[i,:], -1)

        img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)
        cv2.imwrite(config['model_dir']+filename+'.png', img)

    def moveModels(self, newFolderName, originalPath, getEpochMetricsBool=True):
        """
        Moves all important files from an AnalEyeZor generated folder to another.
        @param newFolderName: Name of the new folder.
        @type newFolderName: String
        @param originalPath: Path within runs to the original Folder.
        @type originalPath: String
        @param getEpochMetricsBool: If True, generates Training Metrics from the console.out file.
        @type getEpochMetricsBool: Bool
        @return: None
        @rtype: None
        """
        try:
            os.mkdir(config['log_dir']+newFolderName)
        except:
            print("Folder already exists.")
            return
        if not os.path.isdir(originalPath+"checkpoint/"):
            print("Original Folder not found.")
            return

        #Move all networks
        runNames = os.listdir(originalPath+"checkpoint/")
        for runName in runNames:
            if not runName == ".DS_Store":
                networkNames = os.listdir(originalPath+"checkpoint/"+runName+"/")
                for networkName in networkNames:
                    for modelName in self.modelNames:
                        specialCaseBool = 'pyramidal' in networkName.lower()
                        if modelName.lower() in networkName.lower() and not specialCaseBool:
                            shutil.move(originalPath+"checkpoint/"+runName+"/"+networkName,config['log_dir']+newFolderName+"/checkpoint/"+runName+"/"+networkName)
                        #Special case since CNN and PyramidalCNN both contain the word CNN
                        elif modelName.lower() in networkName.lower() and specialCaseBool and 'pyramidal' in modelName.lower():
                            shutil.move(originalPath + "checkpoint/" + runName + "/" + networkName, config['log_dir'] + newFolderName + "/checkpoint/" + runName + "/" + networkName)
        #Get how many networks exists in this run.
        if os.path.exists(originalPath + "runs.csv"):
            allModelNames = pd.read_csv(originalPath+"runs.csv", usecols=["Model"])
        elif os.path.exists(originalPath + "runs_amplitude.csv"):
            allModelNames = pd.read_csv(originalPath + "runs_amplitude.csv", usecols=["Model"])
        elif os.path.exists(originalPath + "runs_angle.csv"):
            allModelNames = pd.read_csv(originalPath + "runs_angle.csv", usecols=["Model"])
        else:
            print("run not found")
            return

        #Generate Metrics.
        if os.path.exists(originalPath+"console.out") and getEpochMetricsBool:
            i = 1
            multiplier = 2 if config['task'] == "Direction_task" else 1
            with open(os.path.join(originalPath,"console.out")) as f:
                readingValuesBool = False
                metrics = None
                header = "Epoch"
                currentEpoch=0
                for line in f:
                    if i > len(allModelNames)*multiplier:
                        break
                    if "after" in line:
                        readingValuesBool = True
                    if "Epoch" in line:
                        currentEpoch = np.array(re.findall(r"[-+]?\d*\.\d+|\d+", line)).astype(np.float)[0]
                    if "loss" in line and readingValuesBool:
                        metricToAppend = np.array(re.findall(r"[-+]?\d*\.\d+|\d+", line)).astype(np.float)
                        metricToAppend = np.append(currentEpoch,metricToAppend[3:])
                        if metrics is None:
                            metrics = np.expand_dims(metricToAppend,0)
                            if metrics.shape[1] == 2:
                                header = 'Epoch,Loss'
                            elif metrics.shape[1] == 3:
                                header = 'Epoch,Loss,Accuracy'
                            elif metrics.shape[1] == 4:
                                header = 'Epoch,Loss,Accuracy,Val_Loss'
                            else:
                                header = 'Epoch,Loss,Accuracy,Val_Loss,Val_Accuracy'
                        else:
                            metrics = np.concatenate((metrics, np.expand_dims(metricToAppend,0)), axis=0)
                    if "before" in line and readingValuesBool==True:
                        readingValuesBool = False
                        np.savetxt(config['log_dir']+newFolderName+"/"+str(allModelNames.values[i%len(allModelNames)-1][0])+'_{}.csv'.format(i), metrics, fmt='%s',
                                   delimiter=',', header=header, comments='')
                        i+=1
                        metrics = None
                if i <= len(allModelNames)*multiplier:
                    np.savetxt(config['log_dir'] + newFolderName + "/" + str(allModelNames.values[i%len(allModelNames)-1][0]) + '_{}.csv'.format((i-1)%self.numberOfNetworks+1),
                               metrics, fmt='%s',
                               delimiter=',', header=header, comments='')

        #Move important files.
        if os.path.exists(originalPath + "runs_angle.csv"):
            shutil.move(originalPath+"runs_angle.csv", config['log_dir'] + newFolderName)
        if os.path.exists(originalPath + "runs_amplitude.csv"):
            shutil.move(originalPath+"runs_amplitude.csv", config['log_dir'] + newFolderName)
        if os.path.exists(originalPath + "runs.csv"):
            shutil.move(originalPath+"runs.csv", config['log_dir'] + newFolderName)
        if os.path.exists(originalPath + "statistics_amplitude.csv"):
            shutil.move(originalPath + "statistics_amplitude.csv", config['log_dir'] + newFolderName)
        if os.path.exists(originalPath + "statistics.csv"):
            shutil.move(originalPath + "statistics.csv", config['log_dir'] + newFolderName)
        if os.path.exists(originalPath + "statistics_angle.csv"):
            shutil.move(originalPath + "statistics_angle.csv", config['log_dir'] + newFolderName)
        if os.path.exists(originalPath + "info.log"):
            shutil.move(originalPath + "info.log", config['log_dir'] + newFolderName)
        if os.path.exists(originalPath + "config.csv"):
            shutil.move(originalPath + "config.csv", config['log_dir'] + newFolderName)
        if os.path.exists(originalPath + "console.out"):
            shutil.copy(originalPath + "console.out", config['log_dir'] + newFolderName)



        config['load_experiment_dir'] = newFolderName + "/"
        config['model_dir'] = config['log_dir'] + config['load_experiment_dir']
        config['checkpoint_dir'] = config['model_dir'] + 'checkpoint/'
        stamp = str(int(time.time()))
        config['info_log'] = config['model_dir'] + '/' + 'inference_info_' + stamp + '.log'
        config['batches_log'] = config['model_dir'] + '/' + 'inference_batches_' + stamp + '.log'

        self.currentFolderPath = config['log_dir'] + newFolderName + "/"

        if self.electrodes.shape[0] != 129:
            with open(config['model_dir'] + "electrodes" + '.txt', 'w') as f:
                f.write("Electrodes used:\n")
                f.write(np.array2string(self.electrodes,separator=','))

    def colourCode(self, values, electrodes=np.arange(1,130), colourMap="Reds", epsilon = 0.01):
        """
        @param values: Values which are translated to a colour map in a log scale. All values <0 are set to 0.
        @type values: Numpy Array
        @param electrodes: Which indices of the values are modified to the colour map.
        @type electrodes: Numpy Array
        @param colourMap: Matplotlib colour map.
        @type colourMap: String
        @param epsilon: A small number which is added to the values s.t. log(0) does not occur. With AnalEyeZor data, use 1 for Dezibel.
        @type epsilon: Float
        @return:
        @rtype:
        """
        #For Decibel, use epsilon = 1, for good colour visualisation use small epsilon > 0
        values[np.where(values < 0)] = 0
        values = 10*np.log(values + epsilon)
        cmap = cm.get_cmap(colourMap)
        norm = colors.Normalize(vmin=np.min(values[electrodes-1]), vmax=np.max(values[electrodes-1]))
        colours = cmap(norm(values))[:,0:3]
        colours[:,[2, 0]] = colours[:,[0, 2]]
        return colours * 255

    def plotTraining(self, modelFileName, filename='TrainingMetrics', columns=["Loss","Accuracy","Val_Loss","Val_Accuracy"], savePlotBool=True, format="pdf"):
        """
        Visualizes the .csv files generated by the moveModels function.
        @param modelFileName: Exact name of the file which is visualized.
        @type modelFileName: String
        @param filename: Savename for the plot.
        @type filename: String
        @param columns: Which columns are plotted
        @type columns: List of Strings
        @param savePlotBool: If True, saves the plot instead of showing it.
        @type savePlotBool: Bool
        @param format: File format for the save file.
        @type format: String
        @return:
        @rtype:
        """

        columns = ["Epoch"]+columns
        data = pd.read_csv(config['model_dir'] + modelFileName, usecols=columns).to_numpy()
        xAxis = np.arange(int(np.max(data[:,0])))+1

        #Currently Accuracy is mistakenly used for the loss in the console output
        #This renames it.
        if columns[2]=="Accuracy" and len(columns)==3:
            columns[2] = "Val_Loss"
        fig = plt.figure()
        plt.xlabel("Epoch")
        for i in range(1,data.shape[1]):
            plt.plot(xAxis, data[:,i], label=columns[i])
        plt.legend()

        if savePlotBool:
            fig.savefig(config['model_dir'] + filename+".{}".format(format), format=format, transparent=True)
        else:
            plt.show()
        plt.close()

    def topoPlot(self, values, filename='topoPlot', format='pdf',pathForOriginalRelativeToExecutable="./EEGEyeNet/Joels_Files/forPlot/", saveBool=True, cmap='Reds',epsilon=0.01):
        """

        @param values: Array of length 129, where the index + 1 equals the electrode number and a value, which will be colour coded. All values < 0 will be set to 0.
        @type values: Numpy Array
        @param filename: Name of the file as which the plot will be saved.
        @type filename: String
        @param format: Format of the save file.
        @type format: String
        @param pathForOriginalRelativeToExecutable: Depending from where the main.py script is called, this has to be adjusted to be the path relative to the starting file.
        @type pathForOriginalRelativeToExecutable: String
        @param saveBool: If True, the plot will be saved. Else it will be shown.
        @type saveBool: Bool
        @param cmap: Matplotlib colourmap
        @type cmap: String
        @param epsilon: Number to adjust weighting in the log plot. Has to be larger than 0.
        @type epsilon: float
        @return: None
        @rtype: None
        """

        if cmap not in plt.colormaps():
            print("Colourmap does not exist in Matplotlib.")
            cmap = "Reds"
        if epsilon <= 0:
            print("Epsilon too small, using epsilon = 1")
            epsilon = 1
        if values.shape[0] != 129:
            print("Wrong array dimensions")
            return

        electrodePositions = sio.loadmat(pathForOriginalRelativeToExecutable+"lay129_head.mat")['lay129_head']['pos'][0][0]
        outline = sio.loadmat(pathForOriginalRelativeToExecutable+"lay129_head.mat")['lay129_head']['outline'][0][0]
        mask = sio.loadmat(pathForOriginalRelativeToExecutable+"lay129_head.mat")['lay129_head']['mask'][0][0]
        values[np.where(values < 0)] = 0
        values = 10 * np.log(values + epsilon)
        fig = plt.figure(figsize=(7,4.5))
        #Generating outline dictionary for mne topoplot
        outlines = dict()
        outlines["mask_pos"] = (mask[0,0][:,0],mask[0,0][:,1])
        outlines["head"] = (outline[0, 0][:,0],outline[0, 0][:,1])
        outlines["nose"] = (outline[0, 1][:,0],outline[0, 1][:,1])
        outlines["ear_left"] = (outline[0, 2][:,0],outline[0, 2][:,1])
        outlines["ear_right"] = (outline[0, 3][:,0],outline[0, 3][:,1])
        #This cuts out parts of the colour circle
        outlines['clip_radius'] = (0.5,) * 2
        outlines['clip_origin'] = (0,0.07)
        im, cm = mne.viz.plot_topomap(np.squeeze(values),electrodePositions[3:132,:],outlines=outlines,show=False,cmap=cmap)
        clb = fig.colorbar(im)
        if epsilon==1:
            clb.ax.set_title("Loss in Db")
        else:
            clb.ax.set_title("10x Log-Ratio-Loss, eps={}".format(epsilon))
        plt.legend()
        if saveBool:
            fig.savefig(config['model_dir'] + filename + ".{}".format(format), format=format, transparent=True)
        else:
            plt.show()
        plt.close()

    def visualizePrediction(self, modelNames, nrOfruns = 5, nrOfPoints=9, filename='predictionVisualisation', format='pdf',saveBool=True, scale=False):

        trainY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1]
        ids = trainY[:, 0]
        trainIndices, valIndices, testIndices = split(ids, 0.7, 0.15, 0.15)
        if scale:
            trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :, self.electrodes.astype(np.int) - 1]
            logging.info('Standard Scaling')
            scaler = StandardScaler()
            scaler.fit(trainX[trainIndices])
            trainX = scaler.transform(trainX[valIndices])
        else:
            trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][testIndices, :,:]
            trainX = trainX[:, :,self.electrodes.astype(np.int) - 1]
        valY = trainY[testIndices,1:]
        trainX, valY  = trainX[:nrOfPoints], valY[:nrOfPoints,:]
        del trainIndices, valIndices, testIndices, trainY
        if config['task'] == 'Direction_task':
            allpredictions = dict()
            for modelName in modelNames:
                modelAmp = all_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][modelName]
                modelAngle = all_models[config['task']][config['dataset']][config['preprocessing']]['angle'][modelName]
                prediction = np.zeros([nrOfruns, valY[:, 0].shape[0], 2])
                for run in range(nrOfruns):

                    path = config['checkpoint_dir'] + 'run' + str(run+1) + '/'

                    trainerAmp = modelAmp[0](**modelAmp[1])
                    trainerAmp.ensemble.load_file_pattern = re.compile('_amplitude' + modelName + '_nb_*', re.IGNORECASE)
                    trainerAmp.load(path)
                    predictionAmp = np.squeeze(trainerAmp.predict(trainX))

                    trainerAngle = modelAngle[0](**modelAngle[1])
                    trainerAngle.ensemble.load_file_pattern = re.compile('_angle' + modelName + '_nb_*', re.IGNORECASE)
                    trainerAngle.load(path)
                    predictionAngle = np.squeeze(trainerAngle.predict(trainX))

                    prediction[run,:,0] = np.multiply(predictionAmp, np.cos(predictionAngle))
                    prediction[run,:,1] = np.multiply(predictionAmp, np.sin(predictionAngle))
                allpredictions[modelName] = prediction

            truth = np.zeros([predictionAngle.shape[0], 2])
            truth[:,0] = np.multiply(valY[:,0],np.cos(valY[:,1]))
            truth[:, 1] = np.multiply(valY[:,0], np.sin(valY[:,1]))
        elif config['task'] == 'Position_task':
            allpredictions = dict()
            for modelName in modelNames:
                model = all_models[config['task']][config['dataset']][config['preprocessing']][modelName]
                prediction = np.zeros([nrOfruns, valY[:, 0].shape[0],2])
                for run in range(nrOfruns):
                    trainer = model[0](**model[1])
                    trainer.ensemble.load_file_pattern = re.compile(modelName + '_nb_*', re.IGNORECASE)
                    path = config['checkpoint_dir'] + 'run' + str(run+1) + '/'
                    trainer.load(path)
                    prediction[run,:,:] = np.squeeze(trainer.predict(trainX))
                allpredictions[modelName] = prediction
            truth = valY
        else:
            print("Task not yet configured.")
            return

        cmap = cm.get_cmap('nipy_spectral')
        colour = cmap((1+np.arange(len(modelNames)))/len(modelNames))

        fig = plt.figure()
        plt.scatter(truth[:,0],truth[:,1], c='black', marker='x',label="Ground Truth")
        plt.axis('equal')

        for i,modelName in enumerate(modelNames):
            x = np.mean(allpredictions[modelName][:,:, 0], axis=0)
            y = np.mean(allpredictions[modelName][:,:, 1], axis=0)
            colours = np.zeros([x.shape[0],4]) + colour[i]
            plt.scatter(x, y, s=5,c=colours, marker='o',label=modelName)
            radi = np.sqrt(np.square(np.std(allpredictions[modelName][:,:, 0], axis=0)) + np.square(np.std(allpredictions[modelName][:,:, 1], axis=0)))
            ax = fig.gca()

            for j in range(x.shape[0]):
                plt.plot(np.array([x[j],truth[j,0]]),np.array([y[j],truth[j,1]]),c=colour[i])
            colour[i,3] = 0.1
            for j in range(x.shape[0]):
                ax.add_patch(plt.Circle((x[j], y[j]), radi[j], color=colour[i]))

        plt.axhline(0, color='black',linewidth=0.1)
        plt.axvline(0, color='black',linewidth=0.1)

        plt.legend()
        if saveBool:
            fig.savefig(config['model_dir'] + filename + ".{}".format(format), format=format, transparent=True)
        else:
            plt.show()
        plt.close()

    def visualizePredictionDirection(self, modelNames, nrOfruns=5, nrOfPoints=9, filename='predictionVisualisation',
                            format='pdf', saveBool=True, scale=False,
                            pathForOriginalRelativeToExecutable="./Joels_Files/dimensionReductions/",
                            postfix=""):

        if not config['task'] == 'Direction_task':
            print("Function only works for Direction Task.")
            return
        if postfix=="":
            trainY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1]
            ids = trainY[:, 0]
            trainIndices, valIndices, testIndices = split(ids, 0.7, 0.15, 0.15)
            if scale:
                trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :, self.electrodes.astype(np.int) - 1]
                logging.info('Standard Scaling')
                scaler = StandardScaler()
                scaler.fit(trainX[trainIndices])
                trainX = scaler.transform(trainX[valIndices])
            else:
                trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][testIndices, :,:]
                trainX = trainX[:, :,self.electrodes.astype(np.int) - 1]
            valY = trainY[testIndices,1:]
            trainX, valY  = trainX[:nrOfPoints], valY[:nrOfPoints,:]
            del trainIndices, valIndices, testIndices, trainY

        else:
            nrOfPoints=4
            trainX = np.load(pathForOriginalRelativeToExecutable + "customSignals/" + config['task'] + "_with_" + config['dataset'] + "_synchronised_" + config['preprocessing'] + "_" + "StepDirection" + postfix + ".npy")
            valY = np.array([[400,np.pi],[400,np.pi/2],[400,0],[400,-np.pi/2]])
        allpredictions = dict()
        for modelName in modelNames:
            modelAmp = all_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][modelName]
            modelAngle = all_models[config['task']][config['dataset']][config['preprocessing']]['angle'][modelName]
            prediction = np.zeros([nrOfruns, valY[:, 0].shape[0], 2])
            for run in range(nrOfruns):
                path = config['checkpoint_dir'] + 'run' + str(run + 1) + '/'

                trainerAmp = modelAmp[0](**modelAmp[1])
                trainerAmp.ensemble.load_file_pattern = re.compile('_amplitude' + modelName + '_nb_*', re.IGNORECASE)
                trainerAmp.load(path)
                predictionAmp = np.squeeze(trainerAmp.predict(trainX))

                trainerAngle = modelAngle[0](**modelAngle[1])
                trainerAngle.ensemble.load_file_pattern = re.compile('_angle' + modelName + '_nb_*', re.IGNORECASE)
                trainerAngle.load(path)
                predictionAngle = np.squeeze(trainerAngle.predict(trainX))

                prediction[run, :, 0] = predictionAmp
                prediction[run, :, 1] = predictionAngle
            allpredictions[modelName] = prediction

        truth = np.zeros([predictionAngle.shape[0], 2])
        truth[:, 0] = valY[:, 0]
        truth[:, 1] = valY[:, 1]

        cmap = cm.get_cmap('nipy_spectral')
        colour = cmap((1 + np.arange(len(modelNames))) / len(modelNames))
        colourLight = cmap((1 + np.arange(len(modelNames))) / len(modelNames))
        colourLight[:,3] = 0.35

        fig = plt.figure()
        plt.scatter((np.arange(truth.shape[0])-len(modelNames)/2) / nrOfPoints,truth[:, 0], c='black', marker='x', label="Ground Truth")
        plt.axis('auto')
        plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%d px'))
        plt.tick_params(
            axis='x',  # changes apply to the x-axis
            which='both',  # both major and minor ticks are affected
            bottom=False,  # ticks along the bottom edge are off
            top=False,  # ticks along the top edge are off
            labelbottom=False)  # labels along the bottom edge are off
        plt.xlim((-len(modelNames)/2-1.5, len(modelNames)/2+1.5))

        for i, modelName in enumerate(modelNames):
            if i < len(modelNames) / 2:
                pos = i + 1
            else:
                pos = i - len(modelNames)
            y = np.mean(allpredictions[modelName][:, :, 0], axis=0)
            x = (np.arange(y.shape[0])-len(modelNames)/2) / nrOfPoints + pos
            radi = np.sqrt(np.square(np.std(allpredictions[modelName][:, :, 0], axis=0)) + np.square(
                np.std(allpredictions[modelName][:, :, 1], axis=0)))
            plt.errorbar(x, y,yerr=radi, color=colour[i], fmt='.k',label=modelName)
            for j in range(x.shape[0]):
                plt.plot(np.array([x[j], (np.arange(truth.shape[0])[j]-len(modelNames)/2)/nrOfPoints]), np.array([y[j], truth[j, 0]]), c=colourLight[i])

        plt.legend()
        if saveBool:
            fig.savefig(config['model_dir'] + filename+"_Amp" + ".{}".format(format), format=format, transparent=True)
        else:
            plt.show()
        plt.close()
        del fig

        fig = plt.figure()
        plt.scatter(np.multiply(int(len(modelNames)/2)+1+np.arange(nrOfPoints)/nrOfPoints, np.cos(truth[:, 1])), np.multiply(int(len(modelNames)/2)+1+np.arange(nrOfPoints)/nrOfPoints, np.sin(truth[:, 1])), c='black', marker='x',
                    label="Ground Truth")
        plt.axis('equal')
        plt.tick_params(
            axis='both',  # changes apply to both axis
            which='both',  # both major and minor ticks are affected
            bottom=False,  # ticks along the bottom edge are off
            top=False,  # ticks along the top edge are off
            labelbottom=False)  # labels along the bottom edge are off
        plt.yticks([])
        centers = np.linspace(0, 2*np.pi, 9)[:8]
        angleDistance=0
        for j in range(8):
            plt.plot(np.array([0, (len(modelNames)+2)*np.cos(centers[j] + angleDistance)]),np.array([0, (len(modelNames)+2)*np.sin(centers[j] + angleDistance)]), c='black', alpha=0.5)
            plt.text(len(modelNames)*np.cos(centers[j]),len(modelNames)*np.sin(centers[j]),str(round((centers[j]) / np.pi * 180,1))+"°")


        for i, modelName in enumerate(modelNames):
            rad = np.arange(nrOfPoints) / nrOfPoints + i + 1
            if i >= int(len(modelNames)/2):
                rad+=1
            predictions = np.arctan2(np.sin(allpredictions[modelName][:, :, 1]),np.cos(allpredictions[modelName][:, :, 1]))

            means = np.arctan2(np.mean(np.sin(predictions), axis=0),np.mean(np.cos(predictions), axis=0))
            y = rad*np.sin(means)
            x = rad*np.cos(means)
            diff = predictions-means
            radi = np.sqrt(np.mean(np.square(np.arctan2(np.sin(diff),np.cos(diff))), axis=0))
            plt.scatter(x, y, color=colour[i], marker='o', label=modelName)
            for j in range(x.shape[0]):
                theta = np.linspace(-radi[j]/2, radi[j] / 2, 100) + means[j]
                plt.plot(rad[j]*np.cos(theta),rad[j]*np.sin(theta), c=colour[i])
            for j in range(x.shape[0]):
                plt.plot(np.array([x[j], np.multiply(int(len(modelNames)/2)+1+j/nrOfPoints, np.cos(truth[j, 1]))]),
                         np.array([y[j], np.multiply(int(len(modelNames)/2)+1+j/nrOfPoints, np.sin(truth[j, 1]))]), c=colourLight[i])


        plt.legend()
        if saveBool:
            fig.savefig(config['model_dir'] + filename + "_Ang" + ".{}".format(format), format=format, transparent=True)
        else:
            plt.show()
        plt.close()




    def generateTable(self,modelFileName,filename='tableLatex',addNrOfParams=True,caption="Performance of each Network",nrOfDigits=2,scale=1,transposed=True):

        data = pd.read_csv(config['model_dir'] + modelFileName,header=None)
        data = data.astype(str).values.tolist()
        dataCopy = data
        j=0
        deviationIndices = list()
        if transposed:
            while (j<len(data)):
                if not j == len(data) - 1 and 'std' in data[j+1][0].lower():
                    for i in range(1,len(data[0])):
                        dataCopy[j][i] = '$'+str(round(scale*float(data[j][i]), nrOfDigits))+' \pm '+ str(round(scale*float(data[j+1][i]), nrOfDigits))+'$'
                    deviationIndices.append(j+1)
                    j+=2
                else:
                    for i in range(1, len(data[0])):
                        dataCopy[j][i] = data[j][i]
                    j+=1
            deviationIndices = np.asarray(deviationIndices,dtype=int)[::-1]

            for j in deviationIndices:
                for i in np.arange(len(dataCopy[j]))[::-1]:
                    del dataCopy[j][i]
                del dataCopy[j]
            data = dataCopy
        else:
            while (j < len(data[0])):
                if not j == len(data[0]) - 1 and 'std' in data[j + 1][0].lower():
                    for i in range(1, len(data)):
                        dataCopy[i][j] = '$' + str(round(scale * float(data[i][j]), nrOfDigits)) + ' \pm ' + str(round(scale * float(data[i][j+1]), nrOfDigits)) + '$'
                    deviationIndices.append(j + 1)
                    j += 2
                else:
                    for i in range(1, len(data)):
                        dataCopy[i][j] = data[i][j]
                    j += 1
            deviationIndices = np.asarray(deviationIndices, dtype=int)[::-1]

            for i in np.arange(len(dataCopy))[::-1]:
                for j in deviationIndices:
                    del dataCopy[i][j]
                del dataCopy[i]
            data = dataCopy
        if addNrOfParams:
            data[0] = data[0] + ["\#Parameters"]
            for i in range(1,len(data)):
                modelName=data[i][0]
                if 'amplitude' in modelFileName.lower():
                    model = all_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][modelName]
                    saveTrail='_amplitude'
                elif 'angle' in modelFileName.lower():
                    model = all_models[config['task']][config['dataset']][config['preprocessing']]['angle'][modelName]
                    saveTrail = '_angle'
                else:
                    model = all_models[config['task']][config['dataset']][config['preprocessing']][modelName]
                    saveTrail = ''

                path = config['checkpoint_dir'] + 'run' + str(1) + '/'

                trainer = model[0](**model[1])
                trainer.ensemble.load_file_pattern = re.compile(saveTrail + modelName + '_nb_*', re.IGNORECASE)
                trainer.load(path)


                trainableParams = np.sum([np.prod(v.get_shape()) for v in trainer.ensemble.models[0].trainable_weights])
                nonTrainableParams = np.sum([np.prod(v.get_shape()) for v in trainer.ensemble.models[0].non_trainable_weights])
                totalParams = trainableParams + nonTrainableParams
                data[i] = data[i] + [str(totalParams*trainer.ensemble.nb_models)]

        table = Texttable()
        table.set_cols_align(["c"] * len(data[0]))
        table.set_deco(Texttable.HEADER | Texttable.VLINES)
        table.add_rows(data)
        with open(config['model_dir']+filename+'.txt', 'w') as f:
            f.write(latextable.draw_latex(table, caption=caption))

    def combineResults(self,modelFileName,directories,filename="Statistics",columns=["Model","Mean_score","Std_score","Mean_runtime","Std_runtime"],nameColumn="Model",nameStartIndex=0,addNrOfParams=False):
        data = pd.read_csv(config['log_dir'] + directories[0] + modelFileName,index_col=None)

        if addNrOfParams:
            networkList = data[nameColumn].astype(str).values.tolist()
            for i in range(0,len(networkList)):
                modelName=networkList[i]
                if 'amplitude' in modelFileName.lower():
                    model = all_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][modelName]
                    saveTrail='_amplitude'
                elif 'angle' in modelFileName.lower():
                    model = all_models[config['task']][config['dataset']][config['preprocessing']]['angle'][modelName]
                    saveTrail = '_angle'
                else:
                    model = all_models[config['task']][config['dataset']][config['preprocessing']][modelName]
                    saveTrail = ''

                path = config['log_dir'] + directories[0] + "checkpoint/run1/"

                trainer = model[0](**model[1])
                trainer.ensemble.load_file_pattern = re.compile(saveTrail + modelName + '_nb_*', re.IGNORECASE)
                trainer.load(path)


                trainableParams = np.sum([np.prod(v.get_shape()) for v in trainer.ensemble.models[0].trainable_weights])
                nonTrainableParams = np.sum([np.prod(v.get_shape()) for v in trainer.ensemble.models[0].non_trainable_weights])
                totalParams = trainableParams + nonTrainableParams
                data["\#Parameters"] = str(totalParams*trainer.ensemble.nb_models)


        data[nameColumn] = directories[0][nameStartIndex:-1]

        for i in range(1,len(directories)):
            toAppend = pd.read_csv(config['log_dir'] + directories[i] + modelFileName)

            if addNrOfParams:
                networkList = toAppend[nameColumn].astype(str).values.tolist()
                for j in range(0, len(networkList)):
                    modelName = networkList[j]
                    if 'amplitude' in modelFileName.lower():
                        model = all_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][
                            modelName]
                        saveTrail = '_amplitude'
                    elif 'angle' in modelFileName.lower():
                        model = all_models[config['task']][config['dataset']][config['preprocessing']]['angle'][
                            modelName]
                        saveTrail = '_angle'
                    else:
                        model = all_models[config['task']][config['dataset']][config['preprocessing']][modelName]
                        saveTrail = ''

                    path = config['log_dir'] + directories[i] + "checkpoint/run1/"

                    trainer = model[0](**model[1])
                    trainer.ensemble.load_file_pattern = re.compile(saveTrail + modelName + '_nb_*', re.IGNORECASE)
                    trainer.load(path)

                    trainableParams = np.sum(
                        [np.prod(v.get_shape()) for v in trainer.ensemble.models[0].trainable_weights])
                    nonTrainableParams = np.sum(
                        [np.prod(v.get_shape()) for v in trainer.ensemble.models[0].non_trainable_weights])
                    totalParams = trainableParams + nonTrainableParams
                    toAppend["\#Parameters"] = str(totalParams * trainer.ensemble.nb_models)

            toAppend[nameColumn] = directories[i][nameStartIndex:-1]
            data = pd.concat([data, toAppend], ignore_index = True, axis = 0)

        data.to_csv(config['model_dir'] + filename + '.csv', index = False)

    def predictAll(self,postfix="",steps=4,pathForOriginalRelativeToExecutable="./Joels_Files/predictions/",run=1):
        dataY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1][:, 1:]
        pointsPerStep = int(dataY.shape[0] / steps) + 1
        networkElectrodesIndices = self.electrodes.astype(np.int) - 1
        predictions = dict()
        losses = dict()
        for j in range(steps):
            dataX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][j*pointsPerStep:(j+1)*pointsPerStep, :, :]
            dataX = dataX[:, :, networkElectrodesIndices]
            for modelName in self.modelNames:
                if j==0:
                    predictions[modelName] = np.zeros(dataY.shape)
                    nrOfLosses = 1
                    if config['task'] == 'Direction_task':
                        nrOfLosses=2
                    losses[modelName] = np.zeros([dataY.shape[0],nrOfLosses])
                if config['task'] == 'LR_task':
                    model = all_models[config['task']][config['dataset']][config['preprocessing']][modelName]
                    trainer = model[0](**model[1])
                    trainer.ensemble.load_file_pattern = re.compile(modelName + '_nb_*', re.IGNORECASE)
                    path = config['checkpoint_dir'] + 'run' + str(run) + '/'
                    trainer.load(path)
                    predictions[modelName][j*pointsPerStep:(j+1)*pointsPerStep, 0] = np.squeeze(trainer.predict(dataX))
                    losses[modelName][j*pointsPerStep:(j+1)*pointsPerStep] = np.absolute(np.subtract(dataY[j*pointsPerStep:(j+1)*pointsPerStep],predictions[modelName][j*pointsPerStep:(j+1)*pointsPerStep]))

                elif config['task'] == 'Position_task':
                    model = all_models[config['task']][config['dataset']][config['preprocessing']][modelName]
                    trainer = model[0](**model[1])
                    trainer.ensemble.load_file_pattern = re.compile(modelName + '_nb_*', re.IGNORECASE)
                    path = config['checkpoint_dir'] + 'run' + str(run) + '/'
                    trainer.load(path)
                    predictions[modelName][j*pointsPerStep:(j+1)*pointsPerStep, 0] = np.squeeze(trainer.predict(dataX))
                    losses[modelName][j * pointsPerStep:(j + 1) * pointsPerStep] = self.absoluteDistance(
                        dataY[j * pointsPerStep:(j + 1) * pointsPerStep],
                        predictions[modelName][j * pointsPerStep:(j + 1) * pointsPerStep])

                elif config['task'] == 'Direction_task':
                    modelAmp = all_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][
                        modelName]
                    modelAngle = all_models[config['task']][config['dataset']][config['preprocessing']]['angle'][
                        modelName]

                    path = config['checkpoint_dir'] + 'run' + str(run) + '/'

                    trainerAmp = modelAmp[0](**modelAmp[1])
                    trainerAmp.ensemble.load_file_pattern = re.compile('_amplitude' + modelName + '_nb_*',
                                                                       re.IGNORECASE)
                    trainerAmp.load(path)
                    trainerAngle = modelAngle[0](**modelAngle[1])
                    trainerAngle.ensemble.load_file_pattern = re.compile('_angle' + modelName + '_nb_*', re.IGNORECASE)
                    trainerAngle.load(path)

                    predictions[modelName][j*pointsPerStep:(j+1)*pointsPerStep, 0] = np.squeeze(trainerAmp.predict(dataX))
                    predictions[modelName][j*pointsPerStep:(j+1)*pointsPerStep, 1] = np.squeeze(trainerAngle.predict(dataX))

                    losses[modelName][j * pointsPerStep:(j + 1) * pointsPerStep,0] = self.absoluteDistance(
                        dataY[j * pointsPerStep:(j + 1) * pointsPerStep,0],
                        predictions[modelName][j * pointsPerStep:(j + 1) * pointsPerStep,0])
                    losses[modelName][j * pointsPerStep:(j + 1) * pointsPerStep,1] = self.angleError(
                        dataY[j * pointsPerStep:(j + 1) * pointsPerStep,1],
                        predictions[modelName][j * pointsPerStep:(j + 1) * pointsPerStep,1],noMeanBool=True)
            del dataX

        for i,name in enumerate(predictions):
            np.savez(pathForOriginalRelativeToExecutable + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + "_predictions_"+self.modelNames[i]+"_run_"+str(run)+postfix, x=predictions[name], y=dataY, diff=losses[name])




    def plotSignal(self, modelName, electrodes, nrOfLevels=4, colourMap='gist_rainbow', run=1, nrOfPoints=9,
                   pathForOriginalRelativeToExecutable="./Joels_Files/dimensionReductions/",
                   filename='SignalVisualisation_electrode', format='pdf', scale=False, plotTresh=0, maxValue=1000,
                   meanBool=True, componentAnalysis=None, dimensions=5, dataType="", plotMovementBool=False,
                   specificDataIndices=None, splitAngAmpBool = True, percentageThresh=0, scaleModification = 1, offsetModification = 0,
                   plotSignalsSeperatelyBool=False, postfix=""):

        """
        Visualises and colour codes the signals according to the prediction.

        @param modelName: Name of the model used for prediction.
        @type modelName: String
        @param electrodes: Electrode for which a plot of the signal should be generated.
        @type electrodes: Numpy Array
        @param nrOfLevels: Even number, which defines in how many classes the regression problem is split. Has to be at least 2.
        @type nrOfLevels: Integer
        @param colourMap: Matplotlib Colour Map
        @type colourMap: String
        @param run: Which run of the network is used for prediction.
        @type run: Integer
        @param nrOfPoints: How many data points should be plotted.
        @type nrOfPoints: Integer
        @param pathForOriginalRelativeToExecutable: If main.py is not started from its location in EEGEyeNet, this has to be modified.
        @type pathForOriginalRelativeToExecutable: String
        @param filename: As what the plots should be saved.
        @type filename: String
        @param format: Format for the plot.
        @type format: String
        @param scale: Whether the data should be scaled.
        @type scale: Bool
        @param plotTresh: Used for LR Task. Only plots signals which deviate by this amount from the ground truth. Is in [0,1].
        @type plotTresh: Float
        @param maxValue: Maximum Value for the signal in mV.
        @type maxValue: Float
        @param meanBool: Used for LR Task. If True, only the mean signal is plotted.
        @type meanBool: Bool
        @param componentAnalysis: Which component analysis is used on the data. Can be 'PCA'
        @type componentAnalysis: String
        @param dimensions: How many dimension the component analysis uses. Maximum is 500.
        @type dimensions: Integer
        @param activationMaximizationBool: If True, a signal from the activation maximization is used instead of the data set.
        @type activationMaximizationBool: Bool
        @param plotMovementBool: Used for the LR Task. Plots the x Data of the Eyetracker over time.
        @type plotMovementBool: Bool
        @param specificDataIndices: If this is set, instead of the test set these indices are used.
        @type specificDataIndices: Numpy Array
        @param splitAngAmpBool: If True, the plot for the direction task is not combined into an euclidean space.
        @type splitAngAmpBool: Bool
        @param percentageThresh: Used for direction & position task. Since signals are discretized into an arbitrary amount
        of levels, the legend may become cluttered. To avoid this, only show signals whos prediction correspond to atleast this percentage
        of the ground truth class.
        @type percentageThresh: Float
        @param scaleModification: Scale the input data.
        @type scaleModification: Float
        @param offsetModification: Add a scalar to all data points.
        @type offsetModification: Float
        @param plotSignalsSeperatelyBool: Used for LR task. If True, each signal is plotted seperately.
        @type plotSignalsSeperatelyBool: Bool
        @return:
        @rtype:
        """

        #Finds corresponding indices for the electrodes in the data
        intersect, ind_a, electrodes  = np.intersect1d(electrodes, self.electrodes, return_indices=True)
        del intersect, ind_a

        electrodes = np.atleast_1d(electrodes)
        ##################################DataLoading#######################################
        if dataType=="activationMaximization":

            dataX = offsetModification + scaleModification*np.load(pathForOriginalRelativeToExecutable + "ActivationMaximization/" + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + postfix+".npz")['x']
            dataY = np.load(pathForOriginalRelativeToExecutable + "ActivationMaximization/" + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + postfix+".npz")['y']
            ids = np.array([0])

        elif dataType in self.customSignalType:
            dataX = offsetModification + scaleModification*np.load(pathForOriginalRelativeToExecutable + "customSignals/" + config['task'] + "_with_" + config['dataset'] + "_synchronised_" + config['preprocessing'] + "_" + dataType + postfix + ".npy")
            dataY = np.array([0,1])
            if "stepdirection" in dataType.lower():
                dataY = np.array([[400,np.pi],[400,np.pi/2],[400,0],[400,-np.pi/2]])
            ids = np.array([0])
        else:
            Y = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1]
            ids = Y[:, 0]
            trainIndices, valIndices, testIndices = split(ids, 0.7, 0.15, 0.15)
            if specificDataIndices is None:
                specificDataIndices = testIndices
            else:
                specificDataIndices = np.atleast_1d(specificDataIndices)

            networkElectrodesIndices = self.electrodes.astype(np.int) - 1

            #use scale if data has to be scaled, code not tested
            if scale:
                dataX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :, networkElectrodesIndices]
                logging.info('Standard Scaling')
                scaler = StandardScaler()
                scaler.fit(dataX[trainIndices])
                dataX = scaler.transform(dataX[specificDataIndices])
            else:
                dataX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][specificDataIndices, :,:]
                if componentAnalysis == "PCA":
                    dataX = dataX[:, :, :129]
                    dataX = self.pcaDimReduction(dataX, dim=dimensions)
                dataX = dataX[:, :,networkElectrodesIndices]

            dataY = Y[specificDataIndices,1:]
            dataX, dataY  = offsetModification + scaleModification*dataX[:nrOfPoints], dataY[:nrOfPoints,:]

            del trainIndices, valIndices, testIndices, Y, networkElectrodesIndices


        ################################Prediction#############################
        if config['task'] == 'LR_task':
            model = all_models[config['task']][config['dataset']][config['preprocessing']][modelName]
            trainer = model[0](**model[1])
            trainer.ensemble.load_file_pattern = re.compile(modelName + '_nb_*', re.IGNORECASE)
            path = config['checkpoint_dir'] + 'run' + str(run) + '/'
            trainer.load(path)
            #To get unrounded predictions
            trainer.type = 'regressor'
            prediction = np.squeeze(trainer.predict(dataX))
            truth = np.squeeze(dataY)
        elif config['task'] == 'Position_task':
            model = all_models[config['task']][config['dataset']][config['preprocessing']][modelName]
            trainer = model[0](**model[1])
            trainer.ensemble.load_file_pattern = re.compile(modelName + '_nb_*', re.IGNORECASE)
            path = config['checkpoint_dir'] + 'run' + str(run) + '/'
            trainer.load(path)
            prediction = np.squeeze(trainer.predict(dataX))
            truth = np.squeeze(dataY)
        elif config['task'] == 'Direction_task':
            modelAmp = all_models[config['task']][config['dataset']][config['preprocessing']]['amplitude'][modelName]
            modelAngle = all_models[config['task']][config['dataset']][config['preprocessing']]['angle'][modelName]

            path = config['checkpoint_dir'] + 'run' + str(run) + '/'

            trainerAmp = modelAmp[0](**modelAmp[1])
            trainerAmp.ensemble.load_file_pattern = re.compile('_amplitude' + modelName + '_nb_*', re.IGNORECASE)
            trainerAmp.load(path)
            predictionAmp = np.squeeze(trainerAmp.predict(dataX))

            trainerAngle = modelAngle[0](**modelAngle[1])
            trainerAngle.ensemble.load_file_pattern = re.compile('_angle' + modelName + '_nb_*', re.IGNORECASE)
            trainerAngle.load(path)
            predictionAngle = np.squeeze(trainerAngle.predict(dataX))
            prediction = np.zeros([predictionAmp.shape[0],2])
            truth = np.zeros([predictionAmp.shape[0],2])
            if not splitAngAmpBool:
                prediction[:,0] = np.multiply(predictionAmp, np.cos(predictionAngle))
                prediction[:,1] = np.multiply(predictionAmp, np.sin(predictionAngle))
                truth[:,0] = np.multiply(dataY[:,0],np.cos(dataY[:,1]))
                truth[:, 1] = np.multiply(dataY[:,0], np.sin(dataY[:,1]))
            else:
                prediction[:,0] = predictionAmp
                prediction[:,1] = predictionAngle
                truth[:,0] = dataY[:,0]
                truth[:, 1] = dataY[:,1]
        else:
            print("Task not yet configured.")
            return

        ################################GeneratingPlots########################################
        if config['task'] == 'LR_task' and not plotMovementBool:
            cmap = cm.get_cmap(colourMap)
            linSpace = np.arange(1,1001,2)
            #Use this to get rid of predictions close to the truth in the plot
            threshIndices = np.where(np.absolute(prediction - truth) >= plotTresh)
            #An array with which we can determine which class as well as if it was predicted correctly
            indices = np.atleast_1d(2*truth+np.absolute(np.around(prediction) - truth))
            nrCorrectlyPredicted = np.argwhere(indices % 2 == 0).shape[0]
            nrOfTruths = indices.shape[0]
            indices = indices.astype(np.int)[threshIndices]
            dataX=dataX[threshIndices]

            custom_lines = [Line2D([0], [0], color=cmap(np.arange(4)/3)[0], lw=2),
                            Line2D([0], [0], color=cmap(np.arange(4)/3)[1], lw=2),
                            Line2D([0], [0], color=cmap(np.arange(4)/3)[2], lw=2),
                            Line2D([0], [0], color=cmap(np.arange(4)/3)[3], lw=2)]

            if dataType in self.customSignalType or dataType=="activationMaximization":
                for e in electrodes:
                    fig, ax = plt.subplots()
                    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d ms'))
                    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%d mv'))
                    for i in range(dataX.shape[0]):
                        ax.plot(linSpace, np.squeeze(dataX[i,:,e]),lw=1,label="Confidence for 1: {}%".format(round(100*prediction[i],2)))
                    ax.set_ylim(bottom=-maxValue, top=maxValue)
                    ax.legend()
                    fig.savefig(config['model_dir'] + filename + "_El{}.{}".format(str(self.electrodes[e]),format), format=format, transparent=True)
                    plt.close()

            elif meanBool:

                colour = cmap(np.arange(4) / 3)
                for e in electrodes:
                    fig, ax = plt.subplots()
                    ax.legend(custom_lines, ["0-0", "0-1", "1-1", "1-0"])
                    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d ms'))
                    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%d mv'))
                    ax.set_ylim(bottom=-maxValue, top=maxValue)
                    for i in range(4):
                        if (dataX[np.where(indices==i),:,:].ndim and dataX[np.where(indices==i),:,:].size) != 0:
                            averageSignals = np.squeeze(np.mean(dataX[np.where(indices==i),:,:],axis=1))
                            deviationSignals = np.squeeze(np.std(dataX[np.where(indices==i),:,:],axis=1))
                            ax.plot(linSpace, np.squeeze(averageSignals[:, e]), c=np.squeeze(colour[i]), lw=1.5)
                            ax.fill_between(linSpace, averageSignals[:, e] + deviationSignals[:, e], averageSignals[:, e] - deviationSignals[:, e], facecolor=np.squeeze(colour[i]), alpha=0.1)
                    ax.set_title("Acc.: {}%".format(round(nrCorrectlyPredicted / max(nrOfTruths, 1) * 100,2)))
                    fig.savefig(config['model_dir'] + filename + "_El{}.{}".format(str(self.electrodes[e]),format), format=format,transparent=True)
                    plt.close()
            else:
                if nrOfPoints == 1:
                    colour = np.expand_dims(cmap(indices / 3), axis=0)
                else:
                    colour = cmap(indices / 3)
                for e in electrodes:
                    fig, ax = plt.subplots()
                    if nrOfPoints == 1:
                        ax.legend([Line2D([0], [0], color=colour[0], lw=2)], ["Confidence for {}: {}%".format(str(truth),round(100*(truth*prediction+(1-truth)*(1-prediction)),2))])
                    else:
                        ax.legend(custom_lines, ["0-0", "0-1", "1-1", "1-0"])
                    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d ms'))
                    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%d mv'))
                    ax.set_ylim(bottom=-maxValue, top=maxValue)
                    for i in range(dataX.shape[0]):
                        if plotSignalsSeperatelyBool and not nrOfPoints==1:
                            plt.close()
                            fig, ax = plt.subplots()
                            plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d ms'))
                            plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%d mv'))
                            ax.legend([Line2D([0], [0], color=colour[i], lw=2)], ["Confidence for {}: {}%".format(str(truth[i]), round(100 * (truth[i] * prediction[i] + (1 - truth[i]) * (1 - prediction[i])), 2))])
                            ax.plot(linSpace, np.squeeze(dataX[i, :, e]), c=np.squeeze(colour[i]), lw=0.5)
                            ax.set_ylim(bottom=-maxValue, top=maxValue)
                            fig.savefig(config['model_dir'] + filename + "_ID{}_Trial{}_El{}.{}".format(ids[specificDataIndices[i]],specificDataIndices[i],str(self.electrodes[e]), format),format=format, transparent=True)
                        else:
                            ax.plot(linSpace, np.squeeze(dataX[i,:,e]),  c=np.squeeze(colour[i]),lw=0.5)
                    if not plotSignalsSeperatelyBool or nrOfPoints==1:
                        ax.set_title("Acc.: {}%".format(round(nrCorrectlyPredicted/max(nrOfTruths,1)*100,2)))
                        fig.savefig(config['model_dir'] + filename + "_El{}.{}".format(str(self.electrodes[e]),format), format=format, transparent=True)
                    plt.close()

        elif config['task'] == 'LR_task' and plotMovementBool:
            #need different part of data
            del dataX
            dataX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :, -3:]
            dataX = dataX[specificDataIndices]
            dataX = dataX[:nrOfPoints]
            cmap = cm.get_cmap(colourMap)
            linSpace = np.arange(1, 1001, 2)
            #Use this to get rid of predictions close to the truth in the plot
            threshIndices = np.where(np.absolute(prediction - truth) >= plotTresh)
            #An array with which we can determine which class as well as if it was predicted correctly
            indices = np.atleast_1d(2 * truth + np.absolute(np.around(prediction) - truth))
            indices = indices.astype(np.int)[threshIndices]
            dataX = dataX[threshIndices]

            custom_lines = [Line2D([0], [0], color=cmap(np.arange(4) / 3)[0], lw=2),
                            Line2D([0], [0], color=cmap(np.arange(4) / 3)[1], lw=2),
                            Line2D([0], [0], color=cmap(np.arange(4) / 3)[2], lw=2),
                            Line2D([0], [0], color=cmap(np.arange(4) / 3)[3], lw=2)]

            if meanBool:

                colour = cmap(np.arange(4) / 3)
                fig, ax = plt.subplots()
                plt.legend(custom_lines, ["0-0", "0-1", "1-1", "1-0"])
                plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d px'))
                plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%d ms'))
                ax.set_xlim(left=self.displayBoundariesX[0], right=self.displayBoundariesX[1])
                for i in range(4):
                    if (dataX[np.where(indices == i), :, :].ndim and dataX[np.where(indices == i), :, :].size) != 0:
                        averageSignals = np.mean(np.squeeze(dataX[np.where(indices == i), :, :]), axis=0)
                        ax.plot(np.squeeze(averageSignals[:, 0]), linSpace, c=np.squeeze(colour[i]), lw=1.5)
                fig.savefig(config['model_dir'] + filename + "_Eye.{}".format(format), format=format, transparent=True)
                plt.close()
            else:
                fig, ax = plt.subplots()
                plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d px'))
                plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%d ms'))
                ax.set_xlim(left=self.displayBoundariesX[0], right=self.displayBoundariesX[1])
                if nrOfPoints == 1:
                    colour = np.expand_dims(cmap(indices / 3), axis=0)
                    plt.legend([Line2D([0], [0], color=colour[0], lw=2)], ["Confidence for {}: {}%".format(str(truth), round(100 * (truth * prediction + (1 - truth) * (1 - prediction)),2))])
                else:
                    plt.legend(custom_lines, ["0-0", "0-1", "1-1", "1-0"])
                    colour = cmap(indices / 3)
                for i in range(dataX.shape[0]):
                    if plotSignalsSeperatelyBool and not nrOfPoints == 1:
                        plt.close()
                        fig, ax = plt.subplots()
                        plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d px'))
                        plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%d ms'))
                        ax.set_xlim(left=self.displayBoundariesX[0], right=self.displayBoundariesX[1])
                        ax.legend([Line2D([0], [0], color=colour[i], lw=2)], ["Confidence for {}: {}%".format(str(truth[i]), round(100 * (truth[i] * prediction[i] + (1 - truth[i]) * (1 - prediction[i])), 2))])
                        ax.plot(np.squeeze(dataX[i, :, 0]), linSpace, c=np.squeeze(colour[i]), lw=0.5)
                        fig.savefig(config['model_dir'] + filename + "_ID{}_Trial{}_Eye.{}".format(ids[specificDataIndices[i]], specificDataIndices[i],format), format=format, transparent=True)
                    else:
                        ax.plot(np.squeeze(dataX[i, :, 0]), linSpace, c=np.squeeze(colour[i]), lw=0.5)
                if not plotSignalsSeperatelyBool or nrOfPoints == 1:
                    fig.savefig(config['model_dir'] + filename + "_Eye.{}".format(format), format=format, transparent=True)
                plt.close()

        elif config['task'] == 'Position_task' or (config['task'] == 'Direction_task' and not splitAngAmpBool):
            cmap = cm.get_cmap(colourMap)
            colour = cmap(np.arange(nrOfLevels) / (nrOfLevels - 1))
            if nrOfLevels % 2 != 0:
                print("Need even number of levels.")
                return
            centers = np.zeros([int(np.log2(nrOfLevels+2)),int(np.log2(nrOfLevels)),2])
            eps = 50
            #Determine the range of the plots. Important for discretization.
            if config['task'] == 'Position_task':
                minValueX = self.displayBoundariesX[0]
                minValueY = self.displayBoundariesY[0]
                maxValueX = self.displayBoundariesX[1]
                maxValueY = self.displayBoundariesY[1]
            else:
                maxValueX = max(-np.min(truth[:,0]) + eps,np.max(truth[:,0]) - eps)
                maxValueY = max(-np.min(truth[:,1]) + eps,np.max(truth[:,1]) - eps)
                minValueX = -maxValueX
                minValueY = -maxValueY
            #Determine the center points. A Position then is clustered to the nearest center.
            #Centers will be labeled with an integer, starting count from top left and rowwise.
            centers[:,:,0] = np.expand_dims(np.arange(centers.shape[0])*(maxValueX-minValueX)/centers.shape[0] + (maxValueX-minValueX)/(2*centers.shape[0]),axis=1) + minValueX
            centers[:, :, 1] = np.tile((np.arange(centers.shape[1])*(maxValueY - minValueY) /centers.shape[1]  + (maxValueY - minValueY) / (2 * centers.shape[1]))[::-1],(centers.shape[0],1)) + minValueY
            error = self.euclideanDistance(truth,prediction)
            distances = np.zeros(prediction.shape[0]) + np.Infinity
            distancesTruth = distances
            closestCenter = np.zeros(prediction.shape[0],dtype=np.int)
            closestCenterTruth = np.zeros(truth.shape[0],dtype=np.int)
            for j in range(centers.shape[1]):
                for i in range(centers.shape[0]):
                    distance = np.power(prediction[:,0]-centers[i,j,0],2) + np.power(prediction[:,1]-centers[i,j,1],2)
                    closestCenter[np.where(np.squeeze(distance-distances) < 0)] = i + j*centers.shape[0]
                    distances = np.minimum(distances,distance)

                    distance = np.power(truth[:, 0] - centers[i, j, 0], 2) + np.power(truth[:, 1] - centers[i, j, 1], 2)
                    closestCenterTruth[np.where(np.squeeze(distance-distancesTruth) < 0)] = i + j*centers.shape[0]
                    distancesTruth = np.minimum(distancesTruth,distance)

            linSpace = np.arange(1, 1001, 2)
            for e in electrodes:
                fig, ax = plt.subplots(figsize=(8*centers.shape[0],4*centers.shape[1]),dpi=160/np.log2(nrOfLevels))
                ax.set_xlim([minValueX, maxValueX])
                ax.set_ylim([minValueY, maxValueY])
                ax.set_xticks(np.arange(0,centers.shape[0]+1) * (maxValueX - minValueX) / centers.shape[0]+minValueX)
                ax.set_yticks(np.arange(0,centers.shape[1]+1) * (maxValueY - minValueY) / centers.shape[1]+minValueY)
                ax.title.set_text("Average Error Distance: {}px".format(error))
                plt.grid()
                for i in range(centers.shape[1]):
                    for j in range(centers.shape[0]):
                        axes = ax.inset_axes([(centers[j,i,0]-minValueX)/(maxValueX-minValueX) -  0.8/ (2*centers.shape[0]),(centers[j,i,1]-minValueY)/(maxValueY-minValueY) - 0.8 / (2*centers.shape[1]),0.8 / centers.shape[0],0.8  / centers.shape[1]])
                        index=j+i*centers.shape[0]
                        groundTruthSignal = np.squeeze(np.mean(dataX[np.where(closestCenterTruth==index),:,e],axis=1))
                        nrOfTruths = dataX[np.where(closestCenterTruth==index),:,e].shape[1]
                        nrCorrectlyPredicted = 0
                        deviationTruthSignal = np.squeeze(np.std(dataX[np.where(closestCenterTruth==index),:,e],axis=1))
                        axes.plot(linSpace, np.squeeze(groundTruthSignal), c=np.squeeze(colour[index]), lw=1.5, label=str(index)+",({})".format(nrOfTruths))
                        axes.fill_between(linSpace, groundTruthSignal + deviationTruthSignal, groundTruthSignal - deviationTruthSignal,facecolor=np.squeeze(colour[index]), alpha=0.1)
                        for n in range(centers.shape[0]*centers.shape[1]):
                            if np.logical_and(closestCenterTruth == index,closestCenter == n).any():
                                nrPredicted = dataX[np.where(np.logical_and(closestCenterTruth == index,closestCenter == n)), :, e].shape[1]
                                if n == index:
                                    nrCorrectlyPredicted = nrPredicted
                                if 100 * nrPredicted / nrOfTruths >= percentageThresh:
                                    meanSignal = np.squeeze(np.mean(dataX[np.where(np.logical_and(closestCenterTruth == index,closestCenter == n)), :, e], axis=1))
                                    deviationSignal = np.squeeze(np.std(dataX[np.where(np.logical_and(closestCenterTruth == index,closestCenter == n)), :, e], axis=1))
                                    axes.plot(linSpace, np.squeeze(meanSignal), linestyle='--',c=np.squeeze(colour[n]), lw=1, label="{}-{},({}%)".format(index,n ,round(100 * nrPredicted / nrOfTruths)))
                                    axes.fill_between(linSpace, meanSignal + deviationSignal, meanSignal - deviationSignal, facecolor=np.squeeze(colour[n]), alpha=0.1)
                        axes.set_ylim(bottom=-maxValue, top=maxValue)
                        axes.legend()
                        axes.title.set_text("Coord:[{},{}], Acc.: {}%".format(np.around(centers[j,i,0]),np.around(centers[j,i,1]),round(nrCorrectlyPredicted/max(nrOfTruths,1)*100,1)))
                fig.savefig(config['model_dir'] + filename + "_El{}.{}".format(str(self.electrodes[e]), format), format=format,transparent=True)
                plt.close()
        elif config['task'] == 'Direction_task' and splitAngAmpBool:
            cmap = cm.get_cmap(colourMap)
            colour = cmap(np.arange(nrOfLevels) / (nrOfLevels - 1))
            if nrOfLevels % 2 != 0:
                print("Need even number of levels.")
                return
            eps = 10
            maxValueY = np.max(truth[:, 0]) - eps
            centers = np.zeros([nrOfLevels,2])
            #Center[:,0] are the centers for the amplitude task, the [:,1] for the angle task
            centers[:,0] = np.linspace(0,maxValueY,nrOfLevels+1)[:-1]
            centers[:, 0] += (centers[1, 0] - centers[0, 0]) / 2
            centers[:,1] = np.linspace(-np.pi,np.pi,nrOfLevels+1)[:-1]
            errorAmp = self.euclideanDistance(np.expand_dims(truth[:, 0],axis=0),np.expand_dims(prediction[:, 0],axis=0))
            errorAng = self.angleError(truth[:, 1],prediction[:, 1])
            distances = np.zeros((prediction.shape[0],2)) + np.Infinity
            distancesTruth = distances
            closestCenter = np.zeros([prediction.shape[0],2],dtype=np.int)
            closestCenterTruth = np.zeros([truth.shape[0],2],dtype=np.int)

            #Clustering
            for j in range(centers.shape[0]):
                distance = np.zeros(prediction.shape)
                distance[:,0] = np.absolute(prediction[:,0]-centers[j,0])
                distance[:, 1] = self.angleError(centers[j, 1],prediction[:, 1],noMeanBool=True)
                closestCenter[np.where(np.squeeze(distance-distances) < 0)] = j
                distances = np.minimum(distances,distance)

                distance = np.zeros(truth.shape)
                distance[:,0] = np.absolute(truth[:,0]-centers[j,0])
                distance[:, 1] = self.angleError(centers[j, 1],truth[:, 1],noMeanBool=True)
                closestCenterTruth[np.where(np.squeeze(distance-distancesTruth) < 0)] = j
                distancesTruth = np.minimum(distancesTruth,distance)

            linSpace = np.arange(1, 1001, 2)
            for e in electrodes:
                #Amplitude Part
                fig, ax = plt.subplots(figsize=(8,4*centers.shape[0]),dpi=160/np.log2(nrOfLevels))
                ax.get_xaxis().set_visible(False)
                ax.set_ylim([0, maxValueY])
                #centers[1, 0] - centers[0, 0] is used to find the general distance between to center points
                ax.set_yticks(centers[:,0]-(centers[1, 0] - centers[0, 0]) / 2)
                ax.title.set_text("Average Error Distance: {} Pixels".format(errorAmp))
                plt.grid()
                for j in range(centers.shape[0]):
                    axes = ax.inset_axes([0.1,(0.1+j) / nrOfLevels,0.8,0.8 / nrOfLevels])
                    axes.get_xaxis().set_major_formatter(FormatStrFormatter('%d ms'))
                    axes.get_yaxis().set_major_formatter(FormatStrFormatter('%d mv'))
                    meanTruthSignal = np.squeeze(np.mean(dataX[np.where(closestCenterTruth[:,0]==j),:,e],axis=1))
                    nrOfTruths = dataX[np.where(closestCenterTruth[:,0]==j),:,e].shape[1]
                    nrCorrectlyPredicted = 0
                    deviationTruthSignal = np.squeeze(np.std(dataX[np.where(closestCenterTruth[:,0]==j),:,e],axis=1))
                    axes.plot(linSpace, np.squeeze(meanTruthSignal), c=np.squeeze(colour[j]), lw=1.5, label=str(j)+",({})".format(nrOfTruths))
                    axes.fill_between(linSpace, meanTruthSignal + deviationTruthSignal, meanTruthSignal - deviationTruthSignal,facecolor=np.squeeze(colour[j]), alpha=0.1)
                    for n in range(centers.shape[0]):
                        if np.logical_and(closestCenterTruth[:,0] == j,closestCenter[:,0] == n).any():
                            nrPredicted = dataX[np.where(np.logical_and(closestCenterTruth[:,0] == j,closestCenter[:,0] == n)), :, e].shape[1]
                            if n == j:
                                nrCorrectlyPredicted = nrPredicted
                            if 100 * nrPredicted / nrOfTruths >= percentageThresh:
                                if meanBool:
                                    meanSignal = np.squeeze(np.mean(dataX[np.where(np.logical_and(closestCenterTruth[:,0] == j,closestCenter[:,0] == n)), :, e], axis=1))
                                    deviationSignal = np.squeeze(np.std(dataX[np.where(np.logical_and(closestCenterTruth[:,0] == j,closestCenter[:,0] == n)), :, e], axis=1))
                                    axes.plot(linSpace, np.squeeze(meanSignal), linestyle='--',c=np.squeeze(colour[n]), lw=1, label="{}-{},({}%)".format(j,n ,round(100 * nrPredicted / nrOfTruths)))
                                    axes.fill_between(linSpace, meanSignal + deviationSignal, meanSignal - deviationSignal, facecolor=np.squeeze(colour[n]), alpha=0.1)
                                else:
                                    label = np.squeeze(prediction[np.where(np.logical_and(closestCenterTruth[:, 0] == j, closestCenter[:, 0] == n)), 0])
                                    label = np.around(label)
                                    axes.plot(np.expand_dims(linSpace,axis=1), np.transpose(dataX[np.atleast_1d(np.squeeze(np.where(np.logical_and(closestCenterTruth[:,0] == j,closestCenter[:,0] == n)))), :, e]), linestyle='--',lw=0.5,label=label)
                    axes.set_ylim(bottom=-maxValue, top=maxValue)
                    axes.legend()
                    axes.title.set_text("Coord: {}, Acc.: {}%".format(np.around(centers[j,0]),round(nrCorrectlyPredicted/max(nrOfTruths,1)*100,1)))
                fig.savefig(config['model_dir'] + filename + "_Amp_Electrode_{}.{}".format(str(self.electrodes[e]), format), format=format,transparent=True)
                plt.close()

                #Angle Part
                fig, ax = plt.subplots(figsize=(2*centers.shape[0]+2,2*centers.shape[0]+2),dpi=160/max(np.log2(nrOfLevels),3))
                ax.set_xlim([-1, 1])
                ax.set_ylim([-1, 1])
                ax.get_xaxis().set_visible(False)
                ax.get_yaxis().set_visible(False)
                plt.title("Avg Angle Error: {}".format(round(errorAng*100,1)), loc='left')
                angleDistance = (centers[1,1] - centers[0,1]) / 2
                for j in range(centers.shape[0]):
                    plt.plot(np.array([0,np.cos(centers[j,1]+angleDistance)]),np.array([0,np.sin(centers[j,1]+angleDistance)]),c='black',alpha=0.2)
                    #plt.text(0.2*np.cos(centers[j,1]),0.2*np.sin(centers[j,1]),str(round((centers[j,1]) / np.pi * 180,1))+"°")
                    axes = ax.inset_axes([0.4*np.cos(centers[j,1])+0.5-0.75 / nrOfLevels,0.4*np.sin(centers[j,1])+0.5 - 0.75 / nrOfLevels,1.5 / nrOfLevels ,1.5 / nrOfLevels])
                    axes.locator_params(nbins=3)
                    axes.get_xaxis().set_major_formatter(FormatStrFormatter('%d ms'))
                    axes.get_yaxis().set_major_formatter(FormatStrFormatter('%d mv'))
                    meanTruthSignal = np.squeeze(np.mean(dataX[np.where(closestCenterTruth[:,1]==j),:,e],axis=1))
                    nrOfTruths = dataX[np.where(closestCenterTruth[:,1]==j),:,e].shape[1]
                    nrCorrectlyPredicted = 0
                    deviationTruthSignal = np.squeeze(np.std(dataX[np.where(closestCenterTruth[:,1]==j),:,e],axis=1))
                    axes.plot(linSpace, np.squeeze(meanTruthSignal), c=np.squeeze(colour[j]), lw=1.5, label=str(j)+",({})".format(nrOfTruths))
                    axes.fill_between(linSpace, meanTruthSignal + deviationTruthSignal, meanTruthSignal - deviationTruthSignal,facecolor=np.squeeze(colour[j]), alpha=0.1)
                    for n in range(centers.shape[0]):
                        if np.logical_and(closestCenterTruth[:,1] == j,closestCenter[:,1] == n).any():
                            nrPredicted = dataX[np.where(np.logical_and(closestCenterTruth[:,1] == j,closestCenter[:,1] == n)), :, e].shape[1]
                            if n == j:
                                nrCorrectlyPredicted = nrPredicted
                            if 100 * nrPredicted / nrOfTruths >= percentageThresh:
                                if meanBool:
                                    meanSignal = np.squeeze(np.mean(dataX[np.where(np.logical_and(closestCenterTruth[:,1] == j,closestCenter[:,1] == n)), :, e], axis=1))
                                    deviationSignal = np.squeeze(np.std(dataX[np.where(np.logical_and(closestCenterTruth[:,1] == j,closestCenter[:,1] == n)), :, e], axis=1))
                                    axes.plot(linSpace, np.squeeze(meanSignal), linestyle='--',c=np.squeeze(colour[n]), lw=1, label="{}-{},({}%)".format(j,n ,round(100 * nrPredicted / nrOfTruths)))
                                    axes.fill_between(linSpace, meanSignal + deviationSignal, meanSignal - deviationSignal, facecolor=np.squeeze(colour[n]), alpha=0.1)
                                else:
                                    label = np.squeeze(prediction[np.where(np.logical_and(closestCenterTruth[:,1] == j,closestCenter[:,1] == n)),1])
                                    label = np.around(np.arctan2(np.sin(label), np.cos(label)) / np.pi * 180)
                                    axes.plot(np.expand_dims(linSpace,axis=1), np.transpose(dataX[np.atleast_1d(np.squeeze(np.where(np.logical_and(closestCenterTruth[:,1] == j,closestCenter[:,1] == n)))), :, e]), linestyle='--', lw=0.5,label=label)

                    axes.set_ylim(bottom=-maxValue, top=maxValue)
                    axes.legend()
                    axes.title.set_text("Coord: {}°, Acc.: {}%".format(round(centers[j,1] / np.pi * 180,1),round(nrCorrectlyPredicted/max(nrOfTruths,1)*100,1)))
                fig.savefig(config['model_dir'] + filename + "_Ang_Electrode_{}.{}".format(str(self.electrodes[e]), format), format=format,transparent=True)
                plt.close()






    def movie(self,yValue,yDeviation=0,maxValue=100,slowDownFactor=10,filename='movie',cmap='Seismic',pathForOriginalRelativeToExecutable="./Joels_Files/forPlot/"):
        if cmap not in plt.colormaps():
            print("Colourmap does not exist in Matplotlib.")
            cmap = "Reds"

        electrodePositions = sio.loadmat(pathForOriginalRelativeToExecutable+"lay129_head.mat")['lay129_head']['pos'][0][0]
        outline = sio.loadmat(pathForOriginalRelativeToExecutable+"lay129_head.mat")['lay129_head']['outline'][0][0]
        mask = sio.loadmat(pathForOriginalRelativeToExecutable+"lay129_head.mat")['lay129_head']['mask'][0][0]
        fig = plt.figure(figsize=(7,4.5))
        #Generating outline dictionary for mne topoplot
        outlines = dict()
        outlines["mask_pos"] = (mask[0,0][:,0],mask[0,0][:,1])
        outlines["head"] = (outline[0, 0][:,0],outline[0, 0][:,1])
        outlines["nose"] = (outline[0, 1][:,0],outline[0, 1][:,1])
        outlines["ear_left"] = (outline[0, 2][:,0],outline[0, 2][:,1])
        outlines["ear_right"] = (outline[0, 3][:,0],outline[0, 3][:,1])
        #This cuts out parts of the colour circle
        outlines['clip_radius'] = (0.5,) * 2
        outlines['clip_origin'] = (0,0.07)

        trainY = np.squeeze(IOHelper.get_npz_data(config['data_dir'], verbose=True)[1])
        asdf = np.where(np.absolute(trainY[:,1:] - yValue) <= yDeviation)
        trainX = np.squeeze(IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][np.where(np.absolute(trainY[1:] - yValue) <= yDeviation)[0]])
        trainX = np.mean(trainX,axis=0)

        def update(i):
            im, cm = mne.viz.plot_topomap(np.squeeze(trainX[i]),electrodePositions[3:132,:], contours=0,vmin=-maxValue, vmax=maxValue,outlines=outlines,show=False,cmap=cmap)
            return im,

        ani = animation.FuncAnimation(fig, update, frames=500)
        ani.save(os.path.join(config['model_dir'], filename+'.mp4'), fps=int(500 / slowDownFactor), writer='imagemagick')
        #plt.show()
        plt.close()


    def pca(self,scale=False,pathForOriginalRelativeToExecutable="./Joels_Files/dimensionReductions/PCA/"):

        trainY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1]
        ids = trainY[:, 0]
        trainIndices, valIndices, testIndices = split(ids, 0.7, 0.15, 0.15)
        del trainY, valIndices, testIndices

        eigenVectors = np.zeros([500,500,129])
        print("Calculating PCA")
        for i in tqdm(range(129)):
            if scale:
                trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :,i]
                logging.info('Standard Scaling')
                scaler = StandardScaler()
                scaler.fit(trainX[trainIndices])
                trainX = scaler.transform(trainX[trainIndices])
            else:
                trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :, i]
                trainX = trainX[trainIndices]

            covMatrix = np.cov(np.transpose(trainX))
            e,v = tf.linalg.eigh(covMatrix)
            eigenVectors[:,:,i] = v[:,::-1].numpy()
            del e,v,covMatrix,trainX
        np.save(pathForOriginalRelativeToExecutable+config['task']+"_with_"+config['dataset']+"_synchronised_"+config['preprocessing'],eigenVectors)


    def activationMaximization(self,modelName,run=1, filenamePostfix = "", steps=1024, componentAnalysis="", dimensions=10,derivativeWeight = 1,referenceIndices=None, referenceElectrodes=0,initTensor = "Zeros",scale=False,epochs = 5,pathForOriginalRelativeToExecutable="./Joels_Files/dimensionReductions/ActivationMaximization/"):
        referenceElectrodes = np.atleast_1d(referenceElectrodes)

        reference = np.zeros([2, self.inputShape[0], self.inputShape[1], 1])
        if referenceIndices is not None:
            xData = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][np.atleast_1d(referenceIndices), :,:]
            if componentAnalysis == "PCA":
                xData = xData[:, :, :129]
                xData = self.pcaDimReduction(xData, dim=dimensions)
            xData = xData[:,:,self.electrodes.astype(np.int) - 1]
            intersect, ind_a, referenceElectrodes = np.intersect1d(referenceElectrodes, self.electrodes.astype(np.int), return_indices=True)
            del intersect, ind_a
            reference[:,:,np.squeeze(referenceElectrodes)] = xData[:,:,referenceElectrodes]


        if initTensor == "Avg":
            trainY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1]
            ids = trainY[:, 0]
            trainIndices, valIndices, testIndices = split(ids, 0.7, 0.15, 0.15)
            del valIndices, testIndices
            if scale:
                trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :,self.electrodes.astype(np.int) - 1]
                logging.info('Standard Scaling')
                scaler = StandardScaler()
                scaler.fit(trainX[trainIndices])
                trainX = scaler.transform(trainX[trainIndices])
            else:
                trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:, :, self.electrodes.astype(np.int) - 1]
                trainX = trainX[trainIndices]

            averageSignals = np.zeros([2, self.inputShape[0], self.inputShape[1],1])

            for i in range(2):
                averageSignals[i,:,:,0] = np.mean(trainX[np.where(trainY[trainIndices,1]==i)], axis=0)
            del trainX, trainY
        elif initTensor == "Zeros":
            averageSignals = np.zeros([2, self.inputShape[0], self.inputShape[1], 1])
        elif initTensor == "Reference":
            averageSignals = reference
        else:
            averageSignals = tf.random.uniform((2, self.inputShape[0], self.inputShape[1], 1), -100, 100)

        filepattern = re.compile(modelName + '_nb_*', re.IGNORECASE)
        path = config['checkpoint_dir'] + 'run' + str(run) + '/'
        model = None
        for file in os.listdir(path):
            if not filepattern.match(file):
                continue
            else:
                model = keras.models.load_model(path+file)
                break
        if model == None:
            print("Model not found.")
            return
        model.layers[-1].activation = keras.activations.linear
        def loss(output):
            lmbd = 1
            return (lmbd*(1-output[0, 0]),lmbd*output[1, 0])
        confidences = np.zeros(2)
        activationTensors = np.zeros([2, self.inputShape[0], self.inputShape[1]])
        for j in tqdm(range(epochs)):
            visualizeActivation = ActivationMaximization(model)
            activations = visualizeActivation(loss, callbacks=[Progress()],seed_input=averageSignals, regularizers=[customNorm(reference=reference,p=2., pDer=2., derivativeWeight = derivativeWeight)],steps=steps,input_range=(-100.0,100.0))
            confidencesNew = np.array([-1,1])*np.squeeze(model.predict(activations)) + np.array([1,0])
            for i,activation in enumerate(activations):
                if confidencesNew[i] > confidences[i]:
                    confidences[i] = confidencesNew[i]
                    print("Confidence of {} updated to {}.".format(i,round(confidences[i],3)))
                    activationTensors[i] = np.squeeze(activation.astype(np.float32))


        yValues = np.arange(2)

        np.savez(pathForOriginalRelativeToExecutable+config['task']+"_with_"+config['dataset']+"_synchronised_"+config['preprocessing']+filenamePostfix, x = activationTensors, y = yValues)


    def pcaDimReduction(self,data,dim=2,pathForOriginalRelativeToExecutable="./Joels_Files/dimensionReductions/PCA/",transformBack = True):
        v = np.load(pathForOriginalRelativeToExecutable+config['task']+"_with_"+config['dataset']+"_synchronised_"+config['preprocessing']+".npy")
        z = np.transpose(tf.matmul(np.transpose(v),np.transpose(data)).numpy())
        if transformBack:
            z[:,dim:,:] = 0
            returnValue = np.transpose(tf.matmul(np.swapaxes(np.transpose(v),1,2),np.transpose(z)).numpy())
            return returnValue
        else:
            return z[:,:dim,:]

    def findDataPoints(self, type = "UpToDown", targetValueRange = [0,0], thresh = 0,electrode = 32, scale=False, componentAnalysis=None, dimensions=5,pathForOriginalRelativeToExecutable="./Joels_Files/predictions/", model="",run=1,postfix="",lossThresh=0.1,returnAngleBool=False):
        if type == "UpToDown":
            trainY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1]
            if scale:
                ids = trainY[:, 0]
                trainIndices, valIndices, testIndices = split(ids, 0.7, 0.15, 0.15)
                del valIndices, testIndices
                trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0]
                logging.info('Standard Scaling')
                scaler = StandardScaler()
                scaler.fit(trainX[trainIndices])
                meanStart = np.mean(trainX[:, :100, electrode-1], axis=1)
                meanEnd = np.mean(trainX[:, 200:, electrode-1], axis=1)
                conditionIndices = np.argwhere(np.logical_and(meanStart > thresh, meanEnd < thresh))
                del trainX
            elif componentAnalysis == "PCA":
                steps = int(trainY.shape[0] / 3000)
                conditionIndices = None
                for i in tqdm(range(steps)):
                    trainX = np.squeeze(IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][i*1000:(i+1)*1000])
                    trainX = trainX[:, :, :129]
                    trainX = self.pcaDimReduction(trainX, dim=dimensions)
                    meanStart = np.mean(trainX[:, :100, electrode-1], axis=1)
                    meanEnd = np.mean(trainX[:, 200:, electrode-1], axis=1)
                    del trainX
                    if conditionIndices is None:
                        conditionIndices = np.argwhere(np.logical_and(meanStart > thresh, meanEnd < thresh))
                    else:
                        conditionIndices = np.concatenate((conditionIndices,int(1000*i) + np.argwhere(np.logical_and(meanStart > thresh, meanEnd < thresh))))
            else:
                trainX = np.squeeze(IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:,:,electrode-1])

                meanStart = np.mean(trainX[:, :100], axis=1)
                meanEnd = np.mean(trainX[:, 200:], axis=1)
                conditionIndices = np.argwhere(np.logical_and(meanStart > thresh, meanEnd < thresh))
                del trainX
            intersect = np.intersect1d(conditionIndices, np.argwhere(trainY[:, 1] == targetValueRange[0]), return_indices=False)
            return intersect
        elif type == "DownToUp":
            trainY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1]
            if scale:
                ids = trainY[:, 0]
                trainIndices, valIndices, testIndices = split(ids, 0.7, 0.15, 0.15)
                del valIndices, testIndices
                trainX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0]
                logging.info('Standard Scaling')
                scaler = StandardScaler()
                scaler.fit(trainX[trainIndices])
                meanStart = np.mean(trainX[:, :100, electrode-1], axis=1)
                meanEnd = np.mean(trainX[:, 200:, electrode-1], axis=1)
                conditionIndices = np.argwhere(np.logical_and(meanStart < thresh, meanEnd > thresh))
                del trainX
            elif componentAnalysis == "PCA":
                steps = int(trainY.shape[0] / 3000)
                conditionIndices = None
                for i in tqdm(range(steps)):
                    trainX = np.squeeze(IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][i*1000:(i+1)*1000])
                    trainX = trainX[:, :, :129]
                    trainX = self.pcaDimReduction(trainX, dim=dimensions)
                    meanStart = np.mean(trainX[:, :100, electrode-1], axis=1)
                    meanEnd = np.mean(trainX[:, 200:, electrode-1], axis=1)
                    del trainX
                    if conditionIndices is None:
                        conditionIndices = np.argwhere(np.logical_and(meanStart < thresh, meanEnd > thresh))
                    else:
                        conditionIndices = np.concatenate((conditionIndices,int(1000*i) + np.argwhere(np.logical_and(meanStart > thresh, meanEnd < thresh))))
            else:
                trainX = np.squeeze(IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][:,:,electrode-1])

                meanStart = np.mean(trainX[:, :100], axis=1)
                meanEnd = np.mean(trainX[:, 200:], axis=1)
                conditionIndices = np.argwhere(np.logical_and(meanStart < thresh, meanEnd > thresh))
                del trainX
            intersect = np.intersect1d(conditionIndices, np.argwhere(trainY[:, 1] == targetValueRange[0]), return_indices=False)
            return intersect
        elif type == "Missclassified":
            index = 0
            if returnAngleBool:
                index=1
            losses = np.load(pathForOriginalRelativeToExecutable + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + "_predictions_"+model+"_run_"+str(run)+postfix+".npz")["diff"][:,index]
            return np.squeeze(np.argwhere(losses>=lossThresh))
        elif type == "UpAndDown":
            losses = np.load(pathForOriginalRelativeToExecutable + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + "_predictions_" + model + "_run_" + str(run) + postfix + ".npz")["y"][:,1]
            return np.squeeze(np.argwhere(np.logical_and(np.absolute(losses) >= np.pi/2,np.absolute(losses) <= 3*np.pi/4)))
        elif type == "LeftOnly":
            losses = np.load(pathForOriginalRelativeToExecutable + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + "_predictions_" + model + "_run_" + str(run) + postfix + ".npz")["y"][:,1]
            return np.squeeze(np.argwhere(np.absolute(losses) >= 3*np.pi/4))
        elif type == "RightOnly":
            losses = np.load(pathForOriginalRelativeToExecutable + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + "_predictions_" + model + "_run_" + str(run) + postfix + ".npz")["y"][:,1]
            return np.squeeze(np.argwhere(np.absolute(losses) <= 1*np.pi/4))
        elif type == "UpOnly":
            losses = np.load(pathForOriginalRelativeToExecutable + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + "_predictions_" + model + "_run_" + str(run) + postfix + ".npz")["y"][:,1]
            return np.squeeze(np.argwhere(np.logical_and(losses >= np.pi/2,losses <= 3*np.pi/4)))
        elif type == "DownOnly":
            losses = np.load(pathForOriginalRelativeToExecutable + config['task'] + "_with_" + config['dataset'] + "_synchronised_" +config['preprocessing'] + "_predictions_" + model + "_run_" + str(run) + postfix + ".npz")["y"][:,1]
            return np.squeeze(np.argwhere(np.logical_and(-losses >= np.pi/2,-losses <= 3*np.pi/4)))

    def customSignal(self, type="Step", postfix="", noiseStd=0,amplitude=10,pathForOriginalRelativeToExecutable="./Joels_Files/dimensionReductions/customSignals/",
                     turnPoint=150):
        signal = np.zeros([2,self.inputShape[0],self.inputShape[1]])
        noise = np.random.normal(scale=noiseStd,size=signal.shape)
        if type=="Step":
            signal[0,:turnPoint,0] = amplitude
            signal[0, turnPoint:,0] = -amplitude
            signal[0,:turnPoint,1] = -amplitude
            signal[0, turnPoint:,1] = amplitude
            signal[1] = -signal[0]
            signal += noise
        elif type=="ContStep":
            signal[0,:turnPoint-50,0] = amplitude
            signal[0, turnPoint+50:,0] = -amplitude
            signal[0,turnPoint-50:turnPoint+50,0] = np.linspace(amplitude,-amplitude,100)
            signal[0,:turnPoint-50,1] = -amplitude
            signal[0, turnPoint+50:,1] = amplitude
            signal[0, turnPoint-50:turnPoint+50, 1] = np.linspace(-amplitude,amplitude,100)
            signal[1] = -signal[0]
            signal += noise
        elif type=="ContStepConfused":
            signal[0,:turnPoint-50,0] = amplitude
            signal[0, turnPoint+50:,0] = -amplitude
            signal[0,turnPoint-50:turnPoint+50,0] = np.linspace(amplitude,-amplitude,100)
            amplitude = 0.3*amplitude
            signal[0,:turnPoint-50,1] = amplitude
            signal[0, turnPoint+50:,1] = -amplitude
            signal[0, turnPoint-50:turnPoint+50, 1] = np.linspace(amplitude,-amplitude,100)
            signal[1] = -signal[0]
            signal += noise
        elif type=="TripleSlide":
            signal[0,:,0] += amplitude
            signal[0, :, 1] += -amplitude
            turnPoint=100
            signal[0, turnPoint - 50:turnPoint + 50, 0] = np.linspace(amplitude, -amplitude, 100)
            signal[0, turnPoint+150 - 50:turnPoint+150 + 50, 0] = np.linspace(amplitude, -amplitude, 100)
            signal[0, turnPoint+300 - 50:turnPoint+300 + 50, 0] = np.linspace(amplitude, -amplitude, 100)
            signal[0, turnPoint-50:turnPoint+50, 1] = np.linspace(-amplitude,amplitude,100)
            signal[0, turnPoint+150 - 50:turnPoint+150 + 50, 1] = np.linspace(-amplitude, amplitude, 100)
            signal[0, turnPoint+300 - 50:turnPoint+300 + 50, 1] = np.linspace(-amplitude, amplitude, 100)
            signal[1] = -signal[0]
            signal += noise

        elif type=="Constant":
            signal[0,:,0] += amplitude
            signal[0, :, 1] += -amplitude
            signal[1] = -signal[0]
            signal += noise

        elif type=="StepDirection17":
            signal = np.zeros([4, self.inputShape[0], self.inputShape[1]])
            noise = np.random.normal(scale=noiseStd, size=signal.shape)

            signal[0,:turnPoint,0] = amplitude
            signal[0, turnPoint:,0] = -amplitude
            signal[0,:turnPoint,2] = -amplitude
            signal[0, turnPoint:,2] = amplitude
            signal[2] = -signal[0]
            signal[1,:turnPoint,1] = amplitude
            signal[1, turnPoint:,1] = -amplitude
            signal[3] = -signal[1]
            signal += noise

        else:
            print("Type not yet implemented")
            return

        np.save(pathForOriginalRelativeToExecutable + config['task'] + "_with_" + config['dataset'] + "_synchronised_" + config['preprocessing'] + "_" + type + postfix, signal)


    def attentionVisualization(self, modelName, filename, dataType="",method="Saliency",format="pdf",run=1, componentAnalysis="",
                               dimensions=10,dataIndices = np.asarray([0]),maxValue=100,pathForOriginalRelativeToExecutable="./Joels_Files/dimensionReductions/",
                               postfix="",artificialTruths=None, useAngleNetworkBool=False):
        dataIndices = np.atleast_1d(dataIndices)
        if "direction" in config['task'].lower():
            if useAngleNetworkBool:
                filepattern = re.compile('_angle' + modelName + '_nb_*', re.IGNORECASE)
            else:
                filepattern = re.compile('_amplitude' + modelName + '_nb_*', re.IGNORECASE)
        else:
            filepattern = re.compile(modelName + '_nb_*', re.IGNORECASE)
        path = config['checkpoint_dir'] + 'run' + str(run) + '/'
        model = None
        for file in os.listdir(path):
            if not filepattern.match(file):
                continue
            else:
                if useAngleNetworkBool and "direction" in config['task'].lower():
                    scoring2 = (lambda y, y_pred: np.sqrt(
                        np.mean(np.square(np.arctan2(np.sin(y - y_pred.ravel()), np.cos(y - y_pred.ravel()))))))
                    model = keras.models.load_model(path + file,custom_objects = {'angle_loss': scoring2})
                else:
                    model = keras.models.load_model(path + file)
                break

        if model == None:
            print("Model not found.")
            return
        #if "lr" in config['task'].lower():
        model.layers[-1].activation = keras.activations.linear



        if dataType=="activationMaximization":
            dataX = np.load(pathForOriginalRelativeToExecutable + "ActivationMaximization/" + config['task'] + "_with_" + config['dataset'] + "_synchronised_" + config['preprocessing'] + postfix + ".npz")['x']
            dataY = artificialTruths
            if dataY is None:
                dataY = np.zeros(dataX.shape[0])
        elif dataType in self.customSignalType:
            dataX = np.load(pathForOriginalRelativeToExecutable + "customSignals/" + config['task'] + "_with_" + config['dataset'] + "_synchronised_" + config['preprocessing'] + "_" + dataType + postfix + ".npy")
            dataY = artificialTruths
            if dataY is None:
                dataY = np.zeros(dataX.shape[0])
                if "stepdirection" in dataType.lower():
                    dataY = np.array([[400, np.pi], [400, np.pi / 2], [400, 0], [400, -np.pi / 2]])
        else:
            dataX = IOHelper.get_npz_data(config['data_dir'], verbose=True)[0][dataIndices]
            if componentAnalysis == "PCA":
                dataX = dataX[:, :, :129]
                dataX = self.pcaDimReduction(dataX, dim=dimensions)
            dataX = dataX[:, :,self.electrodes.astype(np.int) - 1]
            dataY = IOHelper.get_npz_data(config['data_dir'], verbose=True)[1][dataIndices]
            dataY = dataY[:,1:]

        dataX = np.expand_dims(dataX, axis=3)
        if method=="Saliency":
            def score(output):
                lmbd = 1
                out = lmbd*tf.norm(dataY[:,:output.shape[1]] - output, axis=1)
                if dataY.shape[1]==1:
                    out = lmbd*tf.multiply(-2*(dataY-0.5),output)
                if useAngleNetworkBool:
                    out = tf.sqrt(tf.square(tf.math.atan2(tf.sin(dataY[:,1] - output), tf.cos(dataY[:,1] - output))))
                return tuple(x for x in out)
            saliency = SaliencyCust(model)
            cam = saliency(score, dataX)
        elif method=="ScoreCam":
            def score(output):
                lmbd = 1
                out = lmbd*tf.norm(dataY[:,:output.shape[1]] - output, axis=1)
                if dataY.shape[1]==1:
                    out = lmbd*tf.multiply(-2*(dataY-0.5),output)
                if useAngleNetworkBool:
                    out = tf.sqrt(tf.square(tf.math.atan2(tf.sin(dataY[:,1] - output), tf.cos(dataY[:,1] - output))))
                return tuple(x for x in out)
            scorecam = ScorecamCust(model)
            cam = scorecam(score, dataX, penultimate_layer=-1, max_N=10)
        else:
            print("Method not implemented.")
            return
        linSpace = np.arange(1, 1001, 2)
        for i, title in enumerate(dataIndices):
            for j in range(self.inputShape[1]):
                f, ax = plt.subplots()
                ax.title.set_text("Electrode {} of Trial {}".format(self.electrodes[j],str(title)) + ", GroundTruth: " + np.array2string(dataY[i]))
                ax.set_ylim([-maxValue, maxValue])
                ax.get_xaxis().set_major_formatter(FormatStrFormatter('%d ms'))
                ax.get_yaxis().set_major_formatter(FormatStrFormatter('%d mv'))
                ax.plot(linSpace, dataX[i, :, j], c='black')
                ax.imshow(np.repeat(np.repeat(np.expand_dims(cam[i,:,j],axis=0),2,axis=1),int(maxValue*2),axis=0), extent=[0,1000,int(maxValue),-int(maxValue)],cmap='jet', alpha=0.5,origin='lower')
                plt.savefig(os.path.join(config['model_dir'], filename+'_Sample{}_El{}.'.format(str(i),self.electrodes[j])+format))
                plt.close()

    def absoluteDistance(self,y,yPred):
        return np.absolute(y-yPred)
    def meanSquareError(self,y,yPred):
        return np.sqrt(mean_squared_error(y, yPred.ravel()))

    def angleError(self,y,yPred,noMeanBool=False):
        difference = y - yPred.ravel()
        if noMeanBool:
            return np.absolute(np.arctan2(np.sin(difference), np.cos(difference)))
        return np.sqrt(np.mean(np.square(np.arctan2(np.sin(difference), np.cos(difference)))))

    def euclideanDistance(self,y,yPred):
        return np.sqrt(np.linalg.norm(y - yPred, axis=1).mean())

    def binaryCrossEntropyLoss(self,y,yPred):
        return log_loss(y,yPred, normalize=True)


class customNorm():
    def __init__(self, reference=np.asarray([0]), p=6., pDer=6., derivativeWeight = 1):
        """
        Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded.
            i.e., prevents pixels from taking on very large values.
        Args:
            img_input: 4D image input tensor to the model of shape: `(samples, channels, rows, cols)`
                if data_format='channels_first' or `(samples, rows, cols, channels)` if data_format='channels_last'.
            p: The pth norm to use. If p = float('inf'), infinity-norm will be used.
        """
        self.name = "Custom Loss"
        self.reference = reference
        self.p = p
        self.pDer = pDer
        self.weight = derivativeWeight

    def __call__(self, input_value) -> tf.Tensor:
        value = keras.backend.pow(keras.backend.sum(keras.backend.pow(keras.backend.abs(input_value - self.reference), self.p)), 1. / self.p)
        derivativeValue = keras.backend.pow(keras.backend.sum(keras.backend.pow(keras.backend.abs(convolve1d(input_value,np.array([-1,0,1]), axis=1)), self.pDer)), 1. / self.pDer)
        print(self.normalize(input_value, (value + derivativeValue*self.weight) / (1+self.weight)))
        return self.normalize(input_value, (value + derivativeValue*self.weight) / (1+self.weight))

    def normalize(self, input_tensor, output_tensor):
        """Normalizes the `output_tensor` with respect to `input_tensor` dimensions.
        This makes regularizer weight factor more or less uniform across various input image dimensions.
        Args:
            input_tensor: An tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=
                    channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.
            output_tensor: The tensor to normalize.
        Returns:
            The normalized tensor.
        """
        image_dims = (input_tensor[1:]).shape
        return output_tensor / np.prod(image_dims)

class ScorecamCust(ModelVisualization):
    """Score-CAM and Faster Score-CAM

    References:
        * Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks
          (https://arxiv.org/pdf/1910.01279.pdf)
        * Faster Score-CAM (https://github.com/tabayashi0117/Score-CAM#faster-score-cam)
    """
    def __call__(self,
                 score,
                 seed_input,
                 penultimate_layer=None,
                 seek_penultimate_conv_layer=True,
                 activation_modifier=lambda cam: keras.backend.relu(cam),
                 batch_size=32,
                 max_N=None,
                 training=False,
                 expand_cam=True,
                 normalize_cam=True) -> Union[np.ndarray, list]:
        """Generate score-weighted class activation maps (CAM) by using gradient-free
        visualization method.

        Args:
            score: A :obj:`tf_keras_vis.utils.scores.Score` instance, function or a list of them.
                For example of the Score instance to specify visualizing target::

                    scores = CategoricalScore([1, 294, 413])

                The code above means the same with the one below::

                    score = lambda outputs: (outputs[0][1], outputs[1][294], outputs[2][413])

                When the model has multiple outputs, you MUST pass a list of
                Score instances or functions. For example::

                    from tf_keras_vis.utils.scores import CategoricalScore, InactiveScore
                    score = [
                        CategoricalScore([1, 23]),  # For 1st model output
                        InactiveScore(),            # For 2nd model output
                        ...
                    ]

            seed_input: A tf.Tensor, :obj:`numpy.ndarray` or a list of them to input in the model.
                That's when the model has multiple inputs, you MUST pass a list of tensors.
            penultimate_layer: An index or name of the layer, or the tf.keras.layers.Layer
                instance itself. When None, it means the same with `-1`. If the layer specified by
                this option is not `convolutional` layer, `penultimate_layer` will work as the
                offset to seek `convolutional` layer. Defaults to None.
            seek_penultimate_conv_layer: A bool that indicates whether or not seeks a penultimate
                layer when the layer specified by `penultimate_layer` is not `convolutional` layer.
                Defaults to True.
            activation_modifier: A function to modify the Class Activation Map (CAM). Defaults to
                `lambda cam: K.relu(cam)`.
            batch_size: The number of samples per batch. Defaults to 32.
            max_N: When None or under Zero, run as ScoreCAM. When not None and over Zero of
                Integer, run as Faster-ScoreCAM. Set larger number (or None), need more time to
                visualize CAM but to be able to get clearer attention images. Defaults to None.
            training: A bool that indicates whether the model's training-mode on or off. Defaults
                to False.
            expand_cam: True to resize CAM to the same as input image size. **Note!** When False,
                even if the model has multiple inputs, return only a CAM. Defaults to True.
            normalize_cam: When True, CAM will be normalized. Defaults to True.
            unconnected_gradients: Specifies the gradient value returned when the given input
                tensors are unconnected. Defaults to tf.UnconnectedGradients.NONE.

        Returns:
            An :obj:`numpy.ndarray` or a list of them. They are the Class Activation Maps (CAMs)
            that indicate the `seed_input` regions whose change would most contribute the score
            value.

        Raises:
            :obj:`ValueError`: When there is any invalid arguments.
        """
        # Preparing
        scores = self._get_scores_for_multiple_outputs(score)
        seed_inputs = self._get_seed_inputs_for_multiple_inputs(seed_input)

        # Processing score-cam
        model = ModelModifier(penultimate_layer, seek_penultimate_conv_layer, False)(self.model)
        penultimate_output = model(seed_inputs, training=training)

        if is_mixed_precision(self.model):
            penultimate_output = tf.cast(penultimate_output, self.model.variable_dtype)

        # For efficiently visualizing, extract maps that has a large variance.
        # This excellent idea is devised by tabayashi0117.
        # (see for details: https://github.com/tabayashi0117/Score-CAM#faster-score-cam)
        if max_N is None or max_N <= 0:
            max_N = get_num_of_steps_allowed(penultimate_output.shape[-1])
        elif max_N > 0 and max_N <= penultimate_output.shape[-1]:
            max_N = get_num_of_steps_allowed(max_N)
        else:
            raise ValueError(f"max_N must be 1 or more and {penultimate_output.shape[-1]} or less."
                             f" max_N: {max_N}")
        if max_N < penultimate_output.shape[-1]:
            activation_map_std = tf.math.reduce_std(penultimate_output,
                                                    axis=tuple(
                                                        range(penultimate_output.ndim)[1:-1]),
                                                    keepdims=True)
            _, top_k_indices = tf.math.top_k(activation_map_std, max_N)
            top_k_indices, _ = tf.unique(tf.reshape(top_k_indices, (-1, )))
            penultimate_output = tf.gather(penultimate_output, top_k_indices, axis=-1)
        nsamples = penultimate_output.shape[0]
        channels = 1

        # Upsampling activations
        input_shapes = [seed_input.shape for seed_input in seed_inputs]
        zoom_factors = (zoom_factor(penultimate_output.shape[1:], input_shape[1:-1])
                        for input_shape in input_shapes)
        zoom_factors = ((1, ) + factor + (1, ) for factor in zoom_factors)
        upsampled_activations = [
            zoom(tf.expand_dims(penultimate_output,3), factor, order=1, mode='nearest') for factor in zoom_factors
        ]
        activation_shapes = [activation.shape for activation in upsampled_activations]

        # Normalizing activations
        min_activations = (np.min(activation,
                                  axis=tuple(range(activation.ndim)[1:-1]),
                                  keepdims=True) for activation in upsampled_activations)
        max_activations = (np.max(activation,
                                  axis=tuple(range(activation.ndim)[1:-1]),
                                  keepdims=True) for activation in upsampled_activations)
        normalized_activations = zip(upsampled_activations, min_activations, max_activations)
        normalized_activations = ((activation - _min) / (_max - _min + keras.backend.epsilon())
                                  for activation, _min, _max in normalized_activations)

        # (samples, h, w, c) -> (channels, samples, h, w, c)
        input_templates = (np.tile(seed_input, (channels, ) + (1, ) * len(seed_input.shape))
                           for seed_input in seed_inputs)
        # (samples, h, w, channels) -> (c, samples, h, w, channels)
        masks = (np.tile(mask, (input_shape[-1], ) + (1, ) * len(map_shape)) for mask, input_shape,
                 map_shape in zip(normalized_activations, input_shapes, activation_shapes))
        # (c, samples, h, w, channels) -> (channels, samples, h, w, c)
        masks = (np.transpose(mask, (len(mask.shape) - 1, ) + tuple(range(len(mask.shape)))[1:-1] +
                              (0, )) for mask in masks)
        # Create masked inputs
        masked_seed_inputs = (np.multiply(input_template, mask)
                              for input_template, mask in zip(input_templates, masks))

        # (channels, samples, h, w, c) -> (channels * samples, h, w, c)
        masked_seed_inputs = [
            np.reshape(seed_input, (-1, ) + seed_input.shape[2:])
            for seed_input in masked_seed_inputs
        ]

        # Predicting masked seed-inputs
        preds = self.model.predict(masked_seed_inputs, batch_size=batch_size)
        # (channels * samples, logits) -> (channels, samples, logits)
        preds = (np.reshape(prediction, (channels, nsamples, prediction.shape[-1]))
                 for prediction in listify(preds))

        # Calculating weights
        weights = ([score(keras.backend.softmax(np.transpose(p))) for p in prediction]
                   for score, prediction in zip(scores, preds))
        weights = ([self._validate_weight(s, nsamples) for s in w] for w in weights)
        weights = (np.array(w, dtype=np.float32) for w in weights)
        weights = (np.reshape(w, (channels, nsamples, -1)) for w in weights)
        weights = (np.mean(w, axis=2) for w in weights)
        weights = (np.transpose(w, (1, 0)) for w in weights)
        weights = np.array(list(weights), dtype=np.float32)
        weights = np.sum(weights, axis=0)

        # Generate cam
        cam = keras.backend.batch_dot(tf.expand_dims(penultimate_output,3), weights)
        if activation_modifier is not None:
            cam = activation_modifier(cam)

        if not expand_cam:
            if normalize_cam:
                cam = normalize(cam)
            return cam

        # Visualizing
        zoom_factors = (zoom_factor(cam.shape, X.shape) for X in seed_inputs)
        cam = [zoom(cam, factor, order=1) for factor in zoom_factors]
        if normalize_cam:
            cam = [normalize(x) for x in cam]
        if len(self.model.inputs) == 1 and not isinstance(seed_input, list):
            cam = cam[0]
        return cam

    def _validate_weight(self, score, nsamples):
        invalid = False
        if tf.is_tensor(score) or isinstance(score, np.ndarray):
            invalid = (score.shape[0] != nsamples)
        elif isinstance(score, (list, tuple)):
            invalid = (len(score) != nsamples)
        else:
            invalid = (nsamples != 1)
        if invalid:
            raise ValueError(
                "Score function must return a Tensor, whose the first dimension is "
                "the same as the first dimension of seed_input or "
                ", a list or tuple, whose length is the first dimension of seed_input.")
        else:
            return score

    def _get_seed_inputs_for_multiple_inputs(self, seed_input):
        seed_inputs = listify(seed_input)
        if len(seed_inputs) != len(self.model.inputs):
            raise ValueError(
                f"The model has {len(self.model.inputs)} inputs, "
                f"but the number of seed-inputs tensors you passed is {len(seed_inputs)}.")
        seed_inputs = (x if tf.is_tensor(x) else tf.constant(x) for x in seed_inputs)
        seed_inputs = (tf.expand_dims(x, axis=0) if len(x.shape) == len(tensor.shape[1:]) else x
                       for x, tensor in zip(seed_inputs, self.model.inputs))
        seed_inputs = list(seed_inputs)
        #for i, (x, tensor) in enumerate(zip(seed_inputs, self.model.inputs)):
        #    if len(np.squeeze(x).shape) != len(np.squeeze(tensor).shape):
        #        raise ValueError(
        #            f"seed_input's shape is invalid. model-input index: {i},"
        #            f" model-input shape: {tensor.shape}, seed_input shape: {x.shape}.")
        return seed_inputs

class SaliencyCust(ModelVisualization):
    """Vanilla Saliency and Smooth-Grad

    References:
        * Vanilla Saliency: Deep Inside Convolutional Networks: Visualising Image Classification
          Models and Saliency Maps (https://arxiv.org/pdf/1312.6034)
        * SmoothGrad: removing noise by adding noise (https://arxiv.org/pdf/1706.03825)
    """
    def __call__(self,
                 score,
                 seed_input,
                 smooth_samples=0,
                 smooth_noise=0.20,
                 keepdims=False,
                 gradient_modifier=lambda grads: keras.backend.abs(grads),
                 training=False,
                 normalize_map=True,
                 unconnected_gradients=tf.UnconnectedGradients.NONE) -> Union[np.ndarray, list]:
        """Generate an attention map that appears how output value changes with respect to a small
        change in input image pixels.

        Args:
            score: A :obj:`tf_keras_vis.utils.scores.Score` instance, function or a list of them.
                For example of the Score instance to specify visualizing target::

                    scores = CategoricalScore([1, 294, 413])

                The code above means the same with the one below::

                    score = lambda outputs: (outputs[0][1], outputs[1][294], outputs[2][413])

                When the model has multiple outputs, you MUST pass a list of
                Score instances or functions. For example::

                    from tf_keras_vis.utils.scores import CategoricalScore, InactiveScore
                    score = [
                        CategoricalScore([1, 23]),  # For 1st model output
                        InactiveScore(),            # For 2nd model output
                        ...
                    ]

            seed_input: A tf.Tensor, :obj:`numpy.ndarray` or a list of them to input in the model.
                That's when the model has multiple inputs, you MUST pass a list of tensors.
            smooth_samples (int, optional): The number of calculating gradients iterations. When
                over zero, this method will work as SmoothGrad. When zero, it will work as Vanilla
                Saliency. Defaults to 0.
            smooth_noise: Noise level. Defaults to 0.20.
            keepdims: A bool that indicates whether or not to keep the channels-dimension.
                Defaults to False.
            gradient_modifier: A function to modify gradients. Defaults to None.
            training: A bool that indicates whether the model's training-mode on or off. Defaults
                to False.
            normalize_map (bool, optional): When True, saliency map will be normalized.
                Defaults to True.
            unconnected_gradients: Specifies the gradient value returned when the given input
                tensors are unconnected. Defaults to tf.UnconnectedGradients.NONE.

        Returns:
            An :obj:`numpy.ndarray` or a list of them.
            They are the saliency maps that indicate the `seed_input` regions
            whose change would most contribute the score value.

        Raises:
            :obj:`ValueError`: When there is any invalid arguments.
        """

        # Preparing
        scores = self._get_scores_for_multiple_outputs(score)
        seed_inputs = self._get_seed_inputs_for_multiple_inputs(seed_input)
        # Processing saliency
        if smooth_samples > 0:
            smooth_samples = get_num_of_steps_allowed(smooth_samples)
            seed_inputs = (tf.tile(X, (smooth_samples, ) + tuple(np.ones(X.ndim - 1, np.int)))
                           for X in seed_inputs)
            seed_inputs = (tf.reshape(X, (smooth_samples, -1) + tuple(X.shape[1:]))
                           for X in seed_inputs)
            seed_inputs = ((X, tuple(range(X.ndim)[2:])) for X in seed_inputs)
            seed_inputs = ((X, smooth_noise * (tf.math.reduce_max(X, axis=axis, keepdims=True) -
                                               tf.math.reduce_min(X, axis=axis, keepdims=True)))
                           for X, axis in seed_inputs)
            seed_inputs = (X + np.random.normal(0., sigma, X.shape) for X, sigma in seed_inputs)
            seed_inputs = list(seed_inputs)
            total = (np.zeros_like(X[0]) for X in seed_inputs)
            for i in range(smooth_samples):
                grads = self._get_gradients([X[i] for X in seed_inputs], scores, gradient_modifier,
                                            training, unconnected_gradients)
                total = (total + g for total, g in zip(total, grads))
            grads = [g / smooth_samples for g in total]
        else:
            grads = self._get_gradients(seed_inputs, scores, gradient_modifier, training,
                                        unconnected_gradients)
        # Visualizing
        if not keepdims:
            grads = [np.max(g, axis=-1) for g in grads]
        if normalize_map:
            grads = [normalize(g) for g in grads]
        if len(self.model.inputs) == 1 and not isinstance(seed_input, list):
            grads = grads[0]
        return grads

    def _get_gradients(self, seed_inputs, scores, gradient_modifier, training,
                       unconnected_gradients):
        with tf.GradientTape(watch_accessed_variables=False, persistent=True) as tape:
            tape.watch(seed_inputs)
            outputs = self.model(seed_inputs, training=training)
            outputs = listify(outputs)
            score_values = self._calculate_scores(outputs, scores)
        grads = tape.gradient(score_values,
                              seed_inputs,
                              unconnected_gradients=unconnected_gradients)
        if gradient_modifier is not None:
            grads = [gradient_modifier(g) for g in grads]
        return grads

    def _get_seed_inputs_for_multiple_inputs(self, seed_input):
        seed_inputs = listify(seed_input)
        if len(seed_inputs) != len(self.model.inputs):
            raise ValueError(
                f"The model has {len(self.model.inputs)} inputs, "
                f"but the number of seed-inputs tensors you passed is {len(seed_inputs)}.")
        seed_inputs = (x if tf.is_tensor(x) else tf.constant(x) for x in seed_inputs)
        seed_inputs = (tf.expand_dims(x, axis=0) if len(x.shape) == len(tensor.shape[1:]) else x
                       for x, tensor in zip(seed_inputs, self.model.inputs))
        seed_inputs = list(seed_inputs)
        #for i, (x, tensor) in enumerate(zip(seed_inputs, self.model.inputs)):
        #    if len(x.shape) != len(tensor.shape):
        #        raise ValueError(
        #            f"seed_input's shape is invalid. model-input index: {i},"
         #           f" model-input shape: {tensor.shape}, seed_input shape: {x.shape}.")
        return seed_inputs

